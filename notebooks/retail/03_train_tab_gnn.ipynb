{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN4G4wsIJdm_",
        "outputId": "eaa9f9cf-9361-4287-efc0-6b4e0622a2ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2026.1.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "SqgzSt0cWkHd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_PATH = '/data/processed'\n",
        "OUTPUT_PATH = '/runs/'"
      ],
      "metadata": {
        "id": "O1H0kkFBHe02"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load retail data\n",
        "\n",
        "df_train = pd.read_csv(INPUT_PATH+'synt-df_train.csv')\n",
        "df_val = pd.read_csv(INPUT_PATH+'synt-df_val.csv')\n",
        "df_test = pd.read_csv(INPUT_PATH+'synt-df_test.csv')\n",
        "df_train.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "df_val.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "df_test.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "\n",
        "# fix types\n",
        "\n",
        "for t in ['card_id', 'merchant_id']:\n",
        "    df_train[t] = df_train[t].astype('category')\n",
        "    df_val[t] = df_val[t].astype('category')\n",
        "    df_test[t] = df_test[t].astype('category')\n",
        "\n",
        "tasks = ['unique_merchant_id', 'count_gt_12_card_id',\n",
        "       'count_eq_3_card_id', 'double_card_id_merchant_city_ONLINE',\n",
        "       'duplicate_card_id_merchant_city']"
      ],
      "metadata": {
        "id": "8I9Nnu1iJ9sx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the graph"
      ],
      "metadata": {
        "id": "lZ2ZFH0ZKr85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Collection, Dict, List, Optional, Tuple, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "def df_to_hetero_for_task(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    features: List[str],\n",
        "    task_col: str,\n",
        "    row_encoder=None,  # keep your type if you have it\n",
        "    row_node_type: str = \"row\",\n",
        "    feature_node_prefix: str = \"\",\n",
        "    include_missing: bool = True,\n",
        "    missing_token: str = \"__MISSING__\",\n",
        "    oov_token: str = \"__OOV__\",\n",
        "    add_reverse_edges: bool = True,  # IMPORTANT: default True for row prediction\n",
        "    store_value_strings: bool = False,\n",
        "    x_dtype: torch.dtype = torch.float32,\n",
        "    y_dtype: torch.dtype = torch.float32,\n",
        "    value2idx_in: Optional[Dict[str, Dict[Any, int]]] = None,\n",
        "    use_one_hot: Optional[Union[bool, Collection[str]]] = None,\n",
        "    one_hot_max_size: int = 5000,  # safety guard\n",
        ") -> Tuple[HeteroData, Dict[str, Dict[Any, int]]]:\n",
        "\n",
        "    if task_col not in df.columns:\n",
        "        raise KeyError(f\"task_col='{task_col}' not in df.columns.\")\n",
        "    for f in features:\n",
        "        if f not in df.columns:\n",
        "            raise KeyError(f\"Feature '{f}' not in df.columns.\")\n",
        "\n",
        "    # Normalize one-hot selection\n",
        "    if use_one_hot is True:\n",
        "        one_hot_feats = set(features)\n",
        "    elif use_one_hot:\n",
        "        one_hot_feats = set(use_one_hot)\n",
        "    else:\n",
        "        one_hot_feats = set()\n",
        "\n",
        "    data = HeteroData()\n",
        "    n = len(df)\n",
        "\n",
        "    # Row nodes\n",
        "    data[row_node_type].num_nodes = n\n",
        "    data[row_node_type].y = torch.tensor(df[task_col].to_numpy(), dtype=y_dtype).view(-1)\n",
        "\n",
        "    if row_encoder is not None:\n",
        "        X, _names = row_encoder.transform(df)\n",
        "        data[row_node_type].x = torch.tensor(X, dtype=x_dtype)\n",
        "    # else: leave row.x unset; model should initialize (embedding or constant + type emb)\n",
        "\n",
        "    row_idx = np.arange(n, dtype=np.int64)\n",
        "\n",
        "    # Build or reuse vocab\n",
        "    value2idx: Dict[str, Dict[Any, int]] = {} if value2idx_in is None else value2idx_in\n",
        "\n",
        "    for feat in features:\n",
        "        feat_type = f\"{feature_node_prefix}{feat}\"\n",
        "        rel_type = f\"has_{feat}\"\n",
        "        rev_rel_type = f\"rev_{rel_type}\"\n",
        "\n",
        "        col = df[feat]\n",
        "        if include_missing:\n",
        "            col = col.where(~col.isna(), other=missing_token)\n",
        "\n",
        "        if value2idx_in is None:\n",
        "            # TRAIN: build vocab\n",
        "            uniques = list(pd.unique(col if include_missing else df[feat].dropna()))\n",
        "            if include_missing and missing_token not in uniques:\n",
        "                uniques.append(missing_token)\n",
        "            if oov_token not in uniques:\n",
        "                uniques.append(oov_token)\n",
        "            v2i = {v: i for i, v in enumerate(uniques)}\n",
        "            value2idx[feat] = v2i\n",
        "        else:\n",
        "            # VAL/TEST: reuse vocab\n",
        "            v2i = value2idx_in[feat]\n",
        "            if oov_token not in v2i:\n",
        "                raise ValueError(f\"oov_token='{oov_token}' missing in value2idx_in[{feat!r}]\")\n",
        "\n",
        "        num_vals = len(v2i)\n",
        "        data[feat_type].num_nodes = num_vals\n",
        "\n",
        "        # Optional one-hot for small vocab only\n",
        "        if (feat in one_hot_feats) and (num_vals <= one_hot_max_size):\n",
        "            data[feat_type].x = torch.eye(num_vals, dtype=x_dtype)\n",
        "\n",
        "        if store_value_strings and value2idx_in is None:\n",
        "            data[feat_type].value = list(v2i.keys())\n",
        "\n",
        "        # Edges row -> value (updates value nodes)\n",
        "        oov_idx = v2i[oov_token]\n",
        "        vals = col.to_numpy()\n",
        "\n",
        "        dst = np.fromiter((v2i.get(v, oov_idx) for v in vals), dtype=np.int64, count=len(vals))\n",
        "        edge_index = torch.tensor(np.vstack([row_idx, dst]), dtype=torch.long)\n",
        "        data[(row_node_type, rel_type, feat_type)].edge_index = edge_index\n",
        "\n",
        "        # Reverse edges value -> row (updates row nodes)  ***CRITICAL***\n",
        "        if add_reverse_edges:\n",
        "            data[(feat_type, rev_rel_type, row_node_type)].edge_index = edge_index.flip(0).contiguous()\n",
        "\n",
        "    return data, value2idx\n"
      ],
      "metadata": {
        "id": "DoB15gIz4psB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = ['unique_StockCode',\n",
        "       'count_gt_75_InvoiceNo', 'count_eq_15_InvoiceNo',\n",
        "       'double_InvoiceNo_StockCode_23084', 'duplicate_CustomerID_StockCode']\n",
        "\n",
        "features = ['InvoiceNo', 'StockCode', 'CustomerID']\n",
        "\n",
        "task_col = 'unique_StockCode'\n",
        "data_train, v2i_train = df_to_hetero_for_task(\n",
        "    df_train,\n",
        "    features=features,\n",
        "    task_col=task_col,\n",
        "    row_encoder=None,          # we will attach row.x ourselves\n",
        "    add_reverse_edges=True,\n",
        "    include_missing=True,\n",
        "    missing_token=\"__MISSING__\",\n",
        "    oov_token=\"__OOV__\",       # still useful for missing values\n",
        "    value2idx_in=None,         # IMPORTANT: per-split vocab\n",
        "    use_one_hot=None,\n",
        ")\n",
        "\n",
        "data_val, v2i_val = df_to_hetero_for_task(\n",
        "    df_val,\n",
        "    features=features,\n",
        "    task_col=task_col,\n",
        "    row_encoder=None,\n",
        "    add_reverse_edges=True,\n",
        "    include_missing=True,\n",
        "    missing_token=\"__MISSING__\",\n",
        "    oov_token=\"__OOV__\",\n",
        "    value2idx_in=None,         # IMPORTANT: per-split vocab\n",
        "    use_one_hot=None,\n",
        ")\n",
        "\n",
        "data_test, v2i_test = df_to_hetero_for_task(\n",
        "    df_test,\n",
        "    features=features,\n",
        "    task_col=task_col,\n",
        "    row_encoder=None,\n",
        "    add_reverse_edges=True,\n",
        "    include_missing=True,\n",
        "    missing_token=\"__MISSING__\",\n",
        "    oov_token=\"__OOV__\",\n",
        "    value2idx_in=None,         # IMPORTANT: per-split vocab\n",
        "    use_one_hot=None,\n",
        ")\n"
      ],
      "metadata": {
        "id": "X9yScVoxgw8Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess node level row fearures"
      ],
      "metadata": {
        "id": "It58GZMLijMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class RowFeaturePreprocessor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        text_col: str,\n",
        "        numeric_cols: List[str],\n",
        "        max_tfidf_features: int = 128,\n",
        "        min_df: int = 2,\n",
        "        lowercase: bool = True,\n",
        "    ):\n",
        "        self.text_col = text_col\n",
        "        self.numeric_cols = numeric_cols\n",
        "        self.max_tfidf_features = max_tfidf_features\n",
        "        self.min_df = min_df\n",
        "        self.lowercase = lowercase\n",
        "\n",
        "        self._tfidf = None\n",
        "        self._scaler = None\n",
        "        self._fitted = False\n",
        "\n",
        "    def fit(self, df: pd.DataFrame):\n",
        "        # ---- text ----\n",
        "        text = (\n",
        "            df[self.text_col]\n",
        "            .fillna(\"\")\n",
        "            .astype(str)\n",
        "        )\n",
        "\n",
        "        self._tfidf = TfidfVectorizer(\n",
        "            max_features=self.max_tfidf_features,\n",
        "            min_df=self.min_df,\n",
        "            lowercase=self.lowercase,\n",
        "            ngram_range=(1, 2),\n",
        "        )\n",
        "        self._tfidf.fit(text)\n",
        "\n",
        "        # ---- numeric ----\n",
        "        X_num = df[self.numeric_cols].astype(float).fillna(0.0).to_numpy()\n",
        "        self._scaler = StandardScaler()\n",
        "        self._scaler.fit(X_num)\n",
        "\n",
        "        self._fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, df: pd.DataFrame) -> np.ndarray:\n",
        "        if not self._fitted:\n",
        "            raise RuntimeError(\"RowFeaturePreprocessor must be fitted first\")\n",
        "\n",
        "        # text features\n",
        "        text = (\n",
        "            df[self.text_col]\n",
        "            .fillna(\"\")\n",
        "            .astype(str)\n",
        "        )\n",
        "        X_text = self._tfidf.transform(text).toarray()\n",
        "\n",
        "        # numeric features\n",
        "        X_num = df[self.numeric_cols].astype(float).fillna(0.0).to_numpy()\n",
        "        X_num = self._scaler.transform(X_num)\n",
        "\n",
        "        # concat\n",
        "        return np.hstack([X_num, X_text])\n",
        "\n",
        "    @property\n",
        "    def output_dim(self) -> int:\n",
        "        if not self._fitted:\n",
        "            raise RuntimeError(\"Not fitted\")\n",
        "        return len(self.numeric_cols) + len(self._tfidf.get_feature_names_out())\n"
      ],
      "metadata": {
        "id": "cgMvprjkiRKT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp = RowFeaturePreprocessor(\n",
        "    text_col=\"Description\",\n",
        "    numeric_cols=[\"Quantity\", \"UnitPrice\"],\n",
        "    max_tfidf_features=128,   # adjust as needed\n",
        ").fit(df_train)\n",
        "\n",
        "data_train[\"row\"].x = torch.tensor(pp.transform(df_train), dtype=torch.float32)\n",
        "data_val[\"row\"].x   = torch.tensor(pp.transform(df_val),   dtype=torch.float32)\n",
        "data_test[\"row\"].x  = torch.tensor(pp.transform(df_test),  dtype=torch.float32)\n",
        "\n",
        "print(\"row.x dim:\", data_train[\"row\"].x.shape[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlQqaGsWhXB7",
        "outputId": "8fb05c2d-13cd-4903-9268-d44ecef2f8ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "row.x dim: 130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define GNN and training loop"
      ],
      "metadata": {
        "id": "sX5w8WoK1r8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from typing import Dict, List, Tuple\n",
        "import hashlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import HeteroConv, SAGEConv\n",
        "\n",
        "\n",
        "def _stable_hash_to_bin(s: str, dim: int) -> int:\n",
        "    h = hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
        "    return int(h[:8], 16) % dim\n",
        "\n",
        "\n",
        "# Option 1: HeteroSage\n",
        "class HeteroSAGE_RowX_Structural(nn.Module):\n",
        "    \"\"\"\n",
        "    HeteroGraphSAGE for:\n",
        "      - row nodes with real features: data['row'].x may be None\n",
        "      - categorical/value nodes without x: initialized structurally (type vector + fixed hash)\n",
        "    Intended for inductive (separate-graph) evaluation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata: Tuple[List[str], List[Tuple[str, str, str]]],\n",
        "        *,\n",
        "        max_num_nodes_by_type: Dict[str, int],\n",
        "        row_node_type: str = \"row\",\n",
        "        row_in_dim: int = 0,\n",
        "        hidden_channels: int = 64,\n",
        "        num_layers: int = 2,\n",
        "        dropout: float = 0.1,\n",
        "        sage_aggr: str = \"sum\",\n",
        "        hash_scale: float = 1e-3,\n",
        "        hash_seed: int = 12345,\n",
        "        head_hidden: int = 64,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.node_types, self.edge_types = metadata\n",
        "        self.row_node_type = row_node_type\n",
        "        self.hidden = hidden_channels\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.hash_scale = hash_scale\n",
        "\n",
        "        # --- Type vectors (learnable) ---\n",
        "        self.type_vec = nn.ParameterDict()\n",
        "        for ntype in self.node_types:\n",
        "            p = nn.Parameter(torch.zeros(hidden_channels))\n",
        "            nn.init.normal_(p, mean=0.0, std=0.02)\n",
        "            self.type_vec[ntype] = p\n",
        "\n",
        "        # --- Row feature projection (OPTIONAL) ---\n",
        "        self.row_in_dim = int(row_in_dim)\n",
        "        self.row_proj = nn.Linear(self.row_in_dim, hidden_channels) if self.row_in_dim > 0 else None\n",
        "\n",
        "        # --- Fixed hash buffers per node type (deterministic symmetry breaker) ---\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(hash_seed)\n",
        "\n",
        "        self.hash_buf = nn.ModuleDict()\n",
        "        for ntype in self.node_types:\n",
        "            if ntype not in max_num_nodes_by_type:\n",
        "                raise KeyError(f\"max_num_nodes_by_type missing node type: {ntype!r}\")\n",
        "            nmax = int(max_num_nodes_by_type[ntype])\n",
        "            m = nn.Module()\n",
        "            H = torch.randn((nmax, hidden_channels), generator=g) * hash_scale\n",
        "            m.register_buffer(\"H\", H)\n",
        "            self.hash_buf[ntype] = m\n",
        "\n",
        "        # --- Hetero GraphSAGE layers ---\n",
        "        self.convs = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.convs.append(\n",
        "                HeteroConv(\n",
        "                    {etype: SAGEConv((-1, -1), hidden_channels, aggr=sage_aggr) for etype in self.edge_types},\n",
        "                    aggr=\"sum\",\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # --- Head (binary logits on row nodes) ---\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden_channels, head_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(head_hidden, 1),\n",
        "        )\n",
        "\n",
        "    def _edge_index_dict(self, data: HeteroData):\n",
        "        device = next(self.parameters()).device\n",
        "        out = {}\n",
        "        for etype in self.edge_types:\n",
        "            store = data[etype]\n",
        "            if \"edge_index\" in store and store[\"edge_index\"] is not None and store[\"edge_index\"].numel() > 0:\n",
        "                out[etype] = store[\"edge_index\"].to(device)\n",
        "        return out\n",
        "\n",
        "    def _init_x_dict(self, data: HeteroData) -> Dict[str, torch.Tensor]:\n",
        "        device = next(self.parameters()).device\n",
        "        x_dict: Dict[str, torch.Tensor] = {}\n",
        "\n",
        "        # --- Row nodes: type vec + fixed hash (+ optional projected row.x) ---\n",
        "        n_row = int(data[self.row_node_type].num_nodes)\n",
        "\n",
        "        base_row = self.type_vec[self.row_node_type].to(device).unsqueeze(0).expand(n_row, -1)\n",
        "        H_row = self.hash_buf[self.row_node_type].H[:n_row].to(device)\n",
        "\n",
        "        has_row_x = hasattr(data[self.row_node_type], \"x\") and data[self.row_node_type].x is not None\n",
        "\n",
        "        if has_row_x:\n",
        "            if self.row_proj is None:\n",
        "                raise ValueError(\"row_in_dim=0, but data['row'].x was provided.\")\n",
        "            row_x = data[self.row_node_type].x.to(device)\n",
        "            if row_x.size(-1) != self.row_proj.in_features:\n",
        "                raise ValueError(\n",
        "                    f\"row.x dim mismatch: got {row_x.size(-1)} but model expects {self.row_proj.in_features}.\"\n",
        "                )\n",
        "            x_row = base_row + H_row + self.row_proj(row_x)\n",
        "        else:\n",
        "            # structural-only\n",
        "            x_row = base_row + H_row\n",
        "\n",
        "        x_dict[self.row_node_type] = x_row\n",
        "\n",
        "        # --- Other node types: type vec + fixed hash (no .x assumed) ---\n",
        "        for ntype in self.node_types:\n",
        "            if ntype == self.row_node_type:\n",
        "                continue\n",
        "            n = int(data[ntype].num_nodes)\n",
        "            base = self.type_vec[ntype].to(device).unsqueeze(0).expand(n, -1)\n",
        "            H = self.hash_buf[ntype].H[:n].to(device)\n",
        "            x_dict[ntype] = base + H\n",
        "\n",
        "        return x_dict\n",
        "\n",
        "    def forward(self, data: HeteroData) -> torch.Tensor:\n",
        "        data = data.to(next(self.parameters()).device)\n",
        "\n",
        "        x_dict = self._init_x_dict(data)\n",
        "        edge_index_dict = self._edge_index_dict(data)\n",
        "\n",
        "        for li, conv in enumerate(self.convs):\n",
        "            x_dict = conv(x_dict, edge_index_dict)\n",
        "            if li < self.num_layers - 1:\n",
        "                for ntype in x_dict:\n",
        "                    x_dict[ntype] = F.relu(x_dict[ntype])\n",
        "                    x_dict[ntype] = F.dropout(x_dict[ntype], p=self.dropout, training=self.training)\n",
        "\n",
        "        return self.head(x_dict[self.row_node_type])\n"
      ],
      "metadata": {
        "id": "C--b6b6vjTQu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 2: Sage with Relation Gate\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "import hashlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import HeteroConv, SAGEConv\n",
        "\n",
        "\n",
        "class HeteroSAGE_RowX_RelGate(nn.Module):\n",
        "    \"\"\"\n",
        "    Full-graph hetero GraphSAGE with:\n",
        "      - row nodes: real features (row.x) projected to hidden\n",
        "      - other nodes: type vec + fixed hash init\n",
        "      - SAGEConv(sum) per relation\n",
        "      - learned relation gating across incoming relations per dst node type\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata: Tuple[List[str], List[Tuple[str, str, str]]],\n",
        "        *,\n",
        "        max_num_nodes_by_type: Dict[str, int],\n",
        "        row_node_type: str = \"row\",\n",
        "        row_in_dim: int = 1,\n",
        "        hidden_channels: int = 64,\n",
        "        num_layers: int = 3,\n",
        "        dropout: float = 0.01,\n",
        "        sage_aggr: str = \"sum\",\n",
        "        hash_scale: float = 1e-3,\n",
        "        hash_seed: int = 12345,\n",
        "        rel_gate_hidden: int = 64,\n",
        "        temperature: float = 1.0,\n",
        "        head_hidden: int = 64,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.node_types, self.edge_types = metadata\n",
        "        self.row_node_type = row_node_type\n",
        "        self.hidden = hidden_channels\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.temperature = temperature\n",
        "\n",
        "        # Type vectors\n",
        "        self.type_vec = nn.ParameterDict()\n",
        "        for ntype in self.node_types:\n",
        "            p = nn.Parameter(torch.zeros(hidden_channels))\n",
        "            nn.init.normal_(p, mean=0.0, std=0.02)\n",
        "            self.type_vec[ntype] = p\n",
        "\n",
        "        # Row feature projection\n",
        "        self.row_proj = nn.Linear(row_in_dim, hidden_channels)\n",
        "\n",
        "        # Fixed hash buffers sized by max nodes per type\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(hash_seed)\n",
        "        self.hash_buf = nn.ModuleDict()\n",
        "        for ntype in self.node_types:\n",
        "            nmax = int(max_num_nodes_by_type[ntype])\n",
        "            m = nn.Module()\n",
        "            H = torch.randn((nmax, hidden_channels), generator=g) * hash_scale\n",
        "            m.register_buffer(\"H\", H)\n",
        "            self.hash_buf[ntype] = m\n",
        "\n",
        "        # Convs with aggr=None so we can gate relations ourselves\n",
        "        self.convs = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.convs.append(\n",
        "                HeteroConv(\n",
        "                    {etype: SAGEConv((-1, -1), hidden_channels, aggr=sage_aggr) for etype in self.edge_types},\n",
        "                    aggr=None,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Relation gate per layer and destination type\n",
        "        self.rel_gate = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            md = nn.ModuleDict()\n",
        "            for dst_type in self.node_types:\n",
        "                md[dst_type] = nn.Sequential(\n",
        "                    nn.Linear(2 * hidden_channels, rel_gate_hidden),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(rel_gate_hidden, 1),\n",
        "                )\n",
        "            self.rel_gate.append(md)\n",
        "\n",
        "        # Head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden_channels, head_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(head_hidden, 1),\n",
        "        )\n",
        "\n",
        "    def _edge_index_dict(self, data: HeteroData):\n",
        "        device = next(self.parameters()).device\n",
        "        out = {}\n",
        "        for etype in self.edge_types:\n",
        "            store = data[etype]\n",
        "            if \"edge_index\" in store and store[\"edge_index\"] is not None and store[\"edge_index\"].numel() > 0:\n",
        "                out[etype] = store[\"edge_index\"].to(device)\n",
        "        return out\n",
        "\n",
        "    def _init_x_dict(self, data: HeteroData) -> Dict[str, torch.Tensor]:\n",
        "        device = next(self.parameters()).device\n",
        "        x_dict: Dict[str, torch.Tensor] = {}\n",
        "\n",
        "        for ntype in self.node_types:\n",
        "            n = int(data[ntype].num_nodes)\n",
        "            base = self.type_vec[ntype].to(device).unsqueeze(0).expand(n, -1)\n",
        "            H = self.hash_buf[ntype].H[:n].to(device)\n",
        "\n",
        "            if ntype == self.row_node_type:\n",
        "                if not (hasattr(data[ntype], \"x\") and data[ntype].x is not None):\n",
        "                    raise ValueError(\"Expected data['row'].x for row features.\")\n",
        "                row_x = data[ntype].x.to(device)\n",
        "                x_dict[ntype] = base + H + self.row_proj(row_x)\n",
        "            else:\n",
        "                x_dict[ntype] = base + H\n",
        "\n",
        "        return x_dict\n",
        "\n",
        "    def _rel_gated_aggregate(\n",
        "        self,\n",
        "        layer_i: int,\n",
        "        out_by_rel: Dict[Tuple[str, str, str], torch.Tensor],\n",
        "        x_prev: Dict[str, torch.Tensor],\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        # Group by destination type\n",
        "        grouped = defaultdict(list)\n",
        "        for etype, h in out_by_rel.items():\n",
        "            dst = etype[2]\n",
        "            grouped[dst].append(h)\n",
        "\n",
        "        x_new: Dict[str, torch.Tensor] = {}\n",
        "\n",
        "        for dst_type in self.node_types:\n",
        "            if dst_type not in grouped:\n",
        "                x_new[dst_type] = x_prev[dst_type]\n",
        "                continue\n",
        "\n",
        "            rel_hs = grouped[dst_type]\n",
        "            if len(rel_hs) == 1:\n",
        "                x_new[dst_type] = rel_hs[0]\n",
        "                continue\n",
        "\n",
        "            h_prev = x_prev[dst_type]\n",
        "            gate_net = self.rel_gate[layer_i][dst_type]\n",
        "\n",
        "            scores = []\n",
        "            for h_rel in rel_hs:\n",
        "                scores.append(gate_net(torch.cat([h_rel, h_prev], dim=-1)))  # (N,1)\n",
        "\n",
        "            score_stack = torch.stack(scores, dim=0)  # (R,N,1)\n",
        "            alpha = F.softmax(score_stack / self.temperature, dim=0)\n",
        "            rel_stack = torch.stack(rel_hs, dim=0)  # (R,N,H)\n",
        "            x_new[dst_type] = torch.sum(alpha * rel_stack, dim=0)\n",
        "\n",
        "        return x_new\n",
        "\n",
        "    def forward(self, data: HeteroData) -> torch.Tensor:\n",
        "        data = data.to(next(self.parameters()).device)\n",
        "\n",
        "        x_dict = self._init_x_dict(data)\n",
        "        edge_index_dict = self._edge_index_dict(data)\n",
        "\n",
        "        for li, conv in enumerate(self.convs):\n",
        "            x_prev = x_dict\n",
        "            out_by_rel = conv(x_prev, edge_index_dict)  # etype -> dst tensor\n",
        "            x_dict = self._rel_gated_aggregate(li, out_by_rel, x_prev)\n",
        "\n",
        "            if li < self.num_layers - 1:\n",
        "                for ntype in x_dict:\n",
        "                    x_dict[ntype] = F.relu(x_dict[ntype])\n",
        "                    x_dict[ntype] = F.dropout(x_dict[ntype], p=self.dropout, training=self.training)\n",
        "\n",
        "        return self.head(x_dict[self.row_node_type])\n"
      ],
      "metadata": {
        "id": "78-7WVLwj7rd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training loop"
      ],
      "metadata": {
        "id": "setEmrPJkTvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _predict_probs(model: nn.Module, data, device: torch.device):\n",
        "    model.eval()\n",
        "    d = data.to(device)\n",
        "    logits = model(d).view(-1)\n",
        "    probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "    y = d[\"row\"].y.view(-1).detach().cpu().numpy().astype(np.int32)\n",
        "    return probs, y\n",
        "\n",
        "\n",
        "def _roc_auc_safe(y: np.ndarray, probs: np.ndarray) -> float:\n",
        "    if len(np.unique(y)) < 2:\n",
        "        return float(\"nan\")\n",
        "    return float(roc_auc_score(y, probs))\n",
        "\n",
        "\n",
        "def _best_f1_threshold(y: np.ndarray, probs: np.ndarray, grid: int = 400) -> tuple[float, float]:\n",
        "    ts = np.linspace(0.001, 0.999, grid)\n",
        "    best_t, best_f1 = 0.5, -1.0\n",
        "    for t in ts:\n",
        "        f1 = f1_score(y, (probs >= t).astype(np.int32))\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_t = float(f1), float(t)\n",
        "    return best_t, best_f1\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_at_threshold(model: nn.Module, data, device: torch.device, threshold: float) -> dict:\n",
        "    probs, y = _predict_probs(model, data, device)\n",
        "    y_pred = (probs >= threshold).astype(np.int32)\n",
        "    return {\"f1\": float(f1_score(y, y_pred)), \"roc_auc\": _roc_auc_safe(y, probs)}\n",
        "\n",
        "\n",
        "def train_fullbatch_binary_best_f1(\n",
        "    model: nn.Module,\n",
        "    data_train,\n",
        "    data_val,\n",
        "    data_test=None,\n",
        "    *,\n",
        "    device: torch.device,\n",
        "    lr: float = 5e-4,\n",
        "    weight_decay: float = 1e-6,\n",
        "    epochs: int = 100,\n",
        "    patience: int = 15,\n",
        "    grad_clip: float | None = 1.0,\n",
        "    threshold_grid: int = 400,\n",
        "    print_every: int = 1,\n",
        "):\n",
        "    model = model.to(device)\n",
        "\n",
        "    # pos_weight from TRAIN\n",
        "    y_train_np = data_train[\"row\"].y.detach().cpu().numpy()\n",
        "    pos = float((y_train_np == 1).sum())\n",
        "    neg = float((y_train_np == 0).sum())\n",
        "    pos_weight = neg / max(pos, 1.0)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    best = {\n",
        "        \"val_f1\": -1.0,\n",
        "        \"val_auc\": float(\"nan\"),\n",
        "        \"threshold\": 0.5,\n",
        "        \"epoch\": -1,\n",
        "        \"state_dict\": None,\n",
        "    }\n",
        "    bad_epochs = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        dtr = data_train.to(device)\n",
        "        logits = model(dtr).view(-1)\n",
        "        y = dtr[\"row\"].y.view(-1).float()\n",
        "\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "\n",
        "        if grad_clip is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # choose threshold on VAL\n",
        "        val_probs, val_y = _predict_probs(model, data_val, device)\n",
        "        t_star, val_f1 = _best_f1_threshold(val_y, val_probs, grid=threshold_grid)\n",
        "        val_auc = _roc_auc_safe(val_y, val_probs)\n",
        "\n",
        "        improved = val_f1 > best[\"val_f1\"] + 1e-6\n",
        "        if improved:\n",
        "            best[\"val_f1\"] = float(val_f1)\n",
        "            best[\"val_auc\"] = float(val_auc)\n",
        "            best[\"threshold\"] = float(t_star)\n",
        "            best[\"epoch\"] = epoch\n",
        "            best[\"state_dict\"] = copy.deepcopy({k: v.detach().cpu() for k, v in model.state_dict().items()})\n",
        "            bad_epochs = 0\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            train_metrics = evaluate_at_threshold(model, data_train, device, threshold=t_star)\n",
        "            print(\n",
        "                f\"Epoch {epoch:03d} | loss={loss.item():.4f} | thr*={t_star:.3f} | \"\n",
        "                f\"train_f1={train_metrics['f1']:.4f} train_auc={train_metrics['roc_auc']:.4f} | \"\n",
        "                f\"val_f1={val_f1:.4f} val_auc={val_auc:.4f} | bad={bad_epochs}/{patience}\"\n",
        "            )\n",
        "\n",
        "        if bad_epochs >= patience:\n",
        "            break\n",
        "\n",
        "    if best[\"state_dict\"] is not None:\n",
        "        model.load_state_dict(best[\"state_dict\"])\n",
        "\n",
        "    report = {\n",
        "        \"best_epoch\": best[\"epoch\"],\n",
        "        \"best_val_f1\": best[\"val_f1\"],\n",
        "        \"best_val_auc\": best[\"val_auc\"],\n",
        "        \"best_threshold\": best[\"threshold\"],\n",
        "        \"pos_weight\": float(pos_weight),\n",
        "    }\n",
        "\n",
        "    if data_test is not None:\n",
        "        test_metrics = evaluate_at_threshold(model, data_test, device, threshold=best[\"threshold\"])\n",
        "        report.update({\"test_f1\": test_metrics[\"f1\"], \"test_auc\": test_metrics[\"roc_auc\"]})\n",
        "\n",
        "    return model, report\n"
      ],
      "metadata": {
        "id": "BYbIOsMKlKh1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = (data_train.node_types, data_train.edge_types)\n",
        "row_in_dim = data_train[\"row\"].x.shape[1]\n",
        "\n",
        "max_num_nodes_by_type = {}\n",
        "for ntype in data_train.node_types:\n",
        "    max_num_nodes_by_type[ntype] = max(\n",
        "        int(data_train[ntype].num_nodes),\n",
        "        int(data_val[ntype].num_nodes),\n",
        "        int(data_test[ntype].num_nodes),\n",
        "    )\n",
        "\n",
        "\n",
        "model = HeteroSAGE_RowX_Structural(\n",
        "    metadata,\n",
        "    max_num_nodes_by_type=max_num_nodes_by_type,\n",
        "    row_node_type=\"row\",\n",
        "    row_in_dim=row_in_dim,\n",
        "    hidden_channels=64,\n",
        "    num_layers=3,\n",
        "    dropout=0.0,\n",
        "    sage_aggr=\"sum\",\n",
        "    hash_scale=1e-3,\n",
        "    head_hidden=64,\n",
        ").to(device)\n",
        "\n",
        "\"\"\"model = HeteroSAGE_RowX_RelGate(\n",
        "    metadata,\n",
        "    max_num_nodes_by_type=max_num_nodes_by_type,\n",
        "    row_node_type=\"row\",\n",
        "    row_in_dim=row_in_dim,\n",
        "    hidden_channels=128,\n",
        "    num_layers=2,\n",
        "    dropout=0.0,\n",
        "    sage_aggr=\"sum\",\n",
        "    hash_scale=1e-3,\n",
        "    head_hidden=64,\n",
        ").to(device)\"\"\""
      ],
      "metadata": {
        "id": "NH3Y1PJ1hxrU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "615a772f-ff13-430f-df3a-53342387e721"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model = HeteroSAGE_RowX_RelGate(\\n    metadata,\\n    max_num_nodes_by_type=max_num_nodes_by_type,\\n    row_node_type=\"row\",\\n    row_in_dim=row_in_dim,\\n    hidden_channels=128,\\n    num_layers=2,\\n    dropout=0.0,\\n    sage_aggr=\"sum\",\\n    hash_scale=1e-3,\\n    head_hidden=64,\\n).to(device)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model, report = train_fullbatch_binary_best_f1(\n",
        "    model,\n",
        "    data_train,\n",
        "    data_val,\n",
        "    data_test,\n",
        "    device=device,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-6,\n",
        "    epochs=200,\n",
        ")\n",
        "\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb409I2EkYkq",
        "outputId": "2bde34d7-2bfb-4a41-97d9-9400848728aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | loss=94.4181 | thr*=0.864 | train_f1=0.2719 train_auc=0.5228 | val_f1=0.1314 val_auc=0.4197 | bad=0/15\n",
            "Epoch 002 | loss=15946.8330 | thr*=0.864 | train_f1=0.2716 train_auc=0.5255 | val_f1=0.1332 val_auc=0.4255 | bad=0/15\n",
            "Epoch 003 | loss=10149.8193 | thr*=0.884 | train_f1=0.2717 train_auc=0.5057 | val_f1=0.1317 val_auc=0.4174 | bad=1/15\n",
            "Epoch 004 | loss=637.9578 | thr*=0.879 | train_f1=0.3128 train_auc=0.6236 | val_f1=0.1283 val_auc=0.4085 | bad=2/15\n",
            "Epoch 005 | loss=57.0777 | thr*=0.776 | train_f1=0.3127 train_auc=0.6325 | val_f1=0.1414 val_auc=0.4646 | bad=0/15\n",
            "Epoch 006 | loss=42.6914 | thr*=0.634 | train_f1=0.3261 train_auc=0.6294 | val_f1=0.1321 val_auc=0.4233 | bad=1/15\n",
            "Epoch 007 | loss=16.3681 | thr*=0.636 | train_f1=0.2380 train_auc=0.4832 | val_f1=0.1349 val_auc=0.4249 | bad=2/15\n",
            "Epoch 008 | loss=1671.0797 | thr*=0.574 | train_f1=0.2390 train_auc=0.4881 | val_f1=0.1361 val_auc=0.4292 | bad=3/15\n",
            "Epoch 009 | loss=1444.7683 | thr*=0.534 | train_f1=0.4510 train_auc=0.7559 | val_f1=0.1441 val_auc=0.4847 | bad=0/15\n",
            "Epoch 010 | loss=8.7541 | thr*=0.671 | train_f1=0.2542 train_auc=0.5609 | val_f1=0.2591 val_auc=0.7886 | bad=0/15\n",
            "Epoch 011 | loss=46.2486 | thr*=0.774 | train_f1=0.2700 train_auc=0.6121 | val_f1=0.2504 val_auc=0.7750 | bad=1/15\n",
            "Epoch 012 | loss=22.1130 | thr*=0.766 | train_f1=0.3469 train_auc=0.7259 | val_f1=0.1302 val_auc=0.4080 | bad=2/15\n",
            "Epoch 013 | loss=13.0625 | thr*=0.769 | train_f1=0.3602 train_auc=0.7785 | val_f1=0.1312 val_auc=0.4115 | bad=3/15\n",
            "Epoch 014 | loss=11.5173 | thr*=0.721 | train_f1=0.4686 train_auc=0.8293 | val_f1=0.1315 val_auc=0.4205 | bad=4/15\n",
            "Epoch 015 | loss=5.7908 | thr*=0.581 | train_f1=0.4552 train_auc=0.7730 | val_f1=0.1268 val_auc=0.4165 | bad=5/15\n",
            "Epoch 016 | loss=8.8539 | thr*=0.381 | train_f1=0.3945 train_auc=0.7478 | val_f1=0.1151 val_auc=0.2999 | bad=6/15\n",
            "Epoch 017 | loss=7.0949 | thr*=0.436 | train_f1=0.3151 train_auc=0.6855 | val_f1=0.1777 val_auc=0.6686 | bad=7/15\n",
            "Epoch 018 | loss=5.2594 | thr*=0.524 | train_f1=0.3571 train_auc=0.8075 | val_f1=0.1189 val_auc=0.3261 | bad=8/15\n",
            "Epoch 019 | loss=5.4722 | thr*=0.561 | train_f1=0.3611 train_auc=0.8232 | val_f1=0.1205 val_auc=0.3427 | bad=9/15\n",
            "Epoch 020 | loss=5.1686 | thr*=0.479 | train_f1=0.4124 train_auc=0.8443 | val_f1=0.1161 val_auc=0.3299 | bad=10/15\n",
            "Epoch 021 | loss=1.6904 | thr*=0.306 | train_f1=0.2236 train_auc=0.5993 | val_f1=0.1813 val_auc=0.6629 | bad=11/15\n",
            "Epoch 022 | loss=24.4044 | thr*=0.241 | train_f1=0.1605 train_auc=0.5787 | val_f1=0.1816 val_auc=0.6419 | bad=12/15\n",
            "Epoch 023 | loss=26.4522 | thr*=0.524 | train_f1=0.2689 train_auc=0.6969 | val_f1=0.1879 val_auc=0.6730 | bad=13/15\n",
            "Epoch 024 | loss=8.5520 | thr*=0.281 | train_f1=0.4473 train_auc=0.7734 | val_f1=0.1191 val_auc=0.3898 | bad=14/15\n",
            "Epoch 025 | loss=6.0682 | thr*=0.546 | train_f1=0.3849 train_auc=0.8301 | val_f1=0.1417 val_auc=0.4772 | bad=15/15\n",
            "{'best_epoch': 10, 'best_val_f1': 0.25906735751295334, 'best_val_auc': 0.7885646617213333, 'best_threshold': 0.6713358395989975, 'pos_weight': 5.440058479532164, 'test_f1': 0.20300751879699247, 'test_auc': 0.8094932841413501}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([report])\n",
        "df[\"model\"] = \"Sage\"\n",
        "df[\"task\"] = task_col\n",
        "\n",
        "df.to_csv(OUTPUT_PATH +f\"Retail-GNN-{task_col}.csv\")"
      ],
      "metadata": {
        "id": "qfib1XMVGLWq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bRQxisBVGa7a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}