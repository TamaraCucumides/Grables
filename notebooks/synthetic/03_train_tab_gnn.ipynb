{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN4G4wsIJdm_"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "SqgzSt0cWkHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_PATH = '/data/processed'\n",
        "OUTPUT_PATH = None"
      ],
      "metadata": {
        "id": "wo8xSrSB8Inh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load synth data\n",
        "\n",
        "df_train = pd.read_csv(INPUT_PATH+'synt-df_train.csv')\n",
        "df_val = pd.read_csv(INPUT_PATH+'synt-df_val.csv')\n",
        "df_test = pd.read_csv(INPUT_PATH+'synt-df_test.csv')\n",
        "df_train.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "df_val.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "df_test.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "\n",
        "# fix types\n",
        "\n",
        "for t in ['card_id', 'merchant_id']:\n",
        "    df_train[t] = df_train[t].astype('category')\n",
        "    df_val[t] = df_val[t].astype('category')\n",
        "    df_test[t] = df_test[t].astype('category')\n",
        "\n",
        "tasks = ['unique_merchant_id', 'count_gt_12_card_id',\n",
        "       'count_eq_3_card_id', 'double_card_id_merchant_city_ONLINE',\n",
        "       'duplicate_card_id_merchant_city']"
      ],
      "metadata": {
        "id": "8I9Nnu1iJ9sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the graph"
      ],
      "metadata": {
        "id": "lZ2ZFH0ZKr85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Collection, Dict, List, Optional, Tuple, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "def df_to_hetero_for_task(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    features: List[str],\n",
        "    task_col: str,\n",
        "    row_encoder=None,  # keep your type if you have it\n",
        "    row_node_type: str = \"row\",\n",
        "    feature_node_prefix: str = \"\",\n",
        "    include_missing: bool = True,\n",
        "    missing_token: str = \"__MISSING__\",\n",
        "    oov_token: str = \"__OOV__\",\n",
        "    add_reverse_edges: bool = True,  # IMPORTANT: default True for row prediction\n",
        "    store_value_strings: bool = False,\n",
        "    x_dtype: torch.dtype = torch.float32,\n",
        "    y_dtype: torch.dtype = torch.float32,\n",
        "    value2idx_in: Optional[Dict[str, Dict[Any, int]]] = None,\n",
        "    use_one_hot: Optional[Union[bool, Collection[str]]] = None,\n",
        "    one_hot_max_size: int = 5000,  # safety guard\n",
        ") -> Tuple[HeteroData, Dict[str, Dict[Any, int]]]:\n",
        "\n",
        "    if task_col not in df.columns:\n",
        "        raise KeyError(f\"task_col='{task_col}' not in df.columns.\")\n",
        "    for f in features:\n",
        "        if f not in df.columns:\n",
        "            raise KeyError(f\"Feature '{f}' not in df.columns.\")\n",
        "\n",
        "    # Normalize one-hot selection\n",
        "    if use_one_hot is True:\n",
        "        one_hot_feats = set(features)\n",
        "    elif use_one_hot:\n",
        "        one_hot_feats = set(use_one_hot)\n",
        "    else:\n",
        "        one_hot_feats = set()\n",
        "\n",
        "    data = HeteroData()\n",
        "    n = len(df)\n",
        "\n",
        "    # Row nodes\n",
        "    data[row_node_type].num_nodes = n\n",
        "    data[row_node_type].y = torch.tensor(df[task_col].to_numpy(), dtype=y_dtype).view(-1)\n",
        "\n",
        "    if row_encoder is not None:\n",
        "        X, _names = row_encoder.transform(df)\n",
        "        data[row_node_type].x = torch.tensor(X, dtype=x_dtype)\n",
        "    # else: leave row.x unset; model should initialize (embedding or constant + type emb)\n",
        "\n",
        "    row_idx = np.arange(n, dtype=np.int64)\n",
        "\n",
        "    # Build or reuse vocab\n",
        "    value2idx: Dict[str, Dict[Any, int]] = {} if value2idx_in is None else value2idx_in\n",
        "\n",
        "    for feat in features:\n",
        "        feat_type = f\"{feature_node_prefix}{feat}\"\n",
        "        rel_type = f\"has_{feat}\"\n",
        "        rev_rel_type = f\"rev_{rel_type}\"\n",
        "\n",
        "        col = df[feat]\n",
        "        if include_missing:\n",
        "            col = col.where(~col.isna(), other=missing_token)\n",
        "\n",
        "        if value2idx_in is None:\n",
        "            # TRAIN: build vocab\n",
        "            uniques = list(pd.unique(col if include_missing else df[feat].dropna()))\n",
        "            if include_missing and missing_token not in uniques:\n",
        "                uniques.append(missing_token)\n",
        "            if oov_token not in uniques:\n",
        "                uniques.append(oov_token)\n",
        "            v2i = {v: i for i, v in enumerate(uniques)}\n",
        "            value2idx[feat] = v2i\n",
        "        else:\n",
        "            # VAL/TEST: reuse vocab\n",
        "            v2i = value2idx_in[feat]\n",
        "            if oov_token not in v2i:\n",
        "                raise ValueError(f\"oov_token='{oov_token}' missing in value2idx_in[{feat!r}]\")\n",
        "\n",
        "        num_vals = len(v2i)\n",
        "        data[feat_type].num_nodes = num_vals\n",
        "\n",
        "        # Optional one-hot for small vocab only\n",
        "        if (feat in one_hot_feats) and (num_vals <= one_hot_max_size):\n",
        "            data[feat_type].x = torch.eye(num_vals, dtype=x_dtype)\n",
        "\n",
        "        if store_value_strings and value2idx_in is None:\n",
        "            data[feat_type].value = list(v2i.keys())\n",
        "\n",
        "        # Edges row -> value (updates value nodes)\n",
        "        oov_idx = v2i[oov_token]\n",
        "        vals = col.to_numpy()\n",
        "\n",
        "        dst = np.fromiter((v2i.get(v, oov_idx) for v in vals), dtype=np.int64, count=len(vals))\n",
        "        edge_index = torch.tensor(np.vstack([row_idx, dst]), dtype=torch.long)\n",
        "        data[(row_node_type, rel_type, feat_type)].edge_index = edge_index\n",
        "\n",
        "        # Reverse edges value -> row (updates row nodes)  ***CRITICAL***\n",
        "        if add_reverse_edges:\n",
        "            data[(feat_type, rev_rel_type, row_node_type)].edge_index = edge_index.flip(0).contiguous()\n",
        "\n",
        "    return data, value2idx\n"
      ],
      "metadata": {
        "id": "DoB15gIz4psB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: build hetero graphs for ONE binary task column\n",
        "# Assumes df_train/df_val/df_test are pandas DataFrames\n",
        "\n",
        "\n",
        "features = [\"card_id\", \"merchant_id\", \"merchant_city\"]  # features as nodes\n",
        "task_col = \"duplicate_card_id_merchant_city\"  # pick one task\n",
        "\n",
        "# 1) Build TRAIN graph and vocab (value2idx) from train only\n",
        "data_train, value2idx = df_to_hetero_for_task(\n",
        "    df_train,\n",
        "    features=features,\n",
        "    task_col=task_col,\n",
        "    row_encoder=None,            # skip for your schema\n",
        "    add_reverse_edges=True,      # critical for row prediction\n",
        "    include_missing=True,\n",
        "    missing_token=\"__MISSING__\",\n",
        "    oov_token=\"__OOV__\",\n",
        "    use_one_hot=None,            # recommended: distinguish values via embeddings later\n",
        ")\n",
        "\n",
        "# 2) Build VAL graph using train vocab (unseen values -> __OOV__)\n",
        "data_val, _ = df_to_hetero_for_task(\n",
        "    df_val,\n",
        "    features=features,\n",
        "    task_col=task_col,\n",
        "    row_encoder=None,\n",
        "    add_reverse_edges=True,\n",
        "    include_missing=True,\n",
        "    missing_token=\"__MISSING__\",\n",
        "    oov_token=\"__OOV__\",\n",
        "    value2idx_in=value2idx,      # reuse train vocab\n",
        "    use_one_hot=None,\n",
        ")\n",
        "\n",
        "# 3) Build TEST graph using train vocab\n",
        "data_test, _ = df_to_hetero_for_task(\n",
        "    df_test,\n",
        "    features=features,\n",
        "    task_col=task_col,\n",
        "    row_encoder=None,\n",
        "    add_reverse_edges=True,\n",
        "    include_missing=True,\n",
        "    missing_token=\"__MISSING__\",\n",
        "    oov_token=\"__OOV__\",\n",
        "    value2idx_in=value2idx,      # reuse train vocab\n",
        "    use_one_hot=None,\n",
        ")\n",
        "\n",
        "# Optional sanity checks\n",
        "print(\"Train row nodes:\", data_train[\"row\"].num_nodes)\n",
        "print(\"Val row nodes:  \", data_val[\"row\"].num_nodes)\n",
        "print(\"Test row nodes: \", data_test[\"row\"].num_nodes)\n",
        "\n",
        "print(\"Node types:\", data_train.node_types)\n",
        "print(\"Edge types:\", data_train.edge_types)\n",
        "\n",
        "# y is on row nodes\n",
        "print(\"Train labels shape:\", data_train[\"row\"].y.shape)\n",
        "print(\"Val labels shape:  \", data_val[\"row\"].y.shape)\n",
        "print(\"Test labels shape: \", data_test[\"row\"].y.shape)\n"
      ],
      "metadata": {
        "id": "nnlBxad16bZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define GNN and training loop"
      ],
      "metadata": {
        "id": "sX5w8WoK1r8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import HeteroConv, SAGEConv\n",
        "\n",
        "\n",
        "class StructuralInductiveHeteroSAGE(nn.Module):\n",
        "    \"\"\"\n",
        "    Inductive hetero GraphSAGE designed for structural/degree-like logical tasks.\n",
        "\n",
        "    Key properties:\n",
        "      - No learned ID embeddings for value nodes (avoids memorization across splits).\n",
        "      - Constant type embeddings per node type.\n",
        "      - Tiny noise added at init to break exact symmetry (critical to avoid constant outputs).\n",
        "      - SAGEConv with SUM aggregation (preserves count information).\n",
        "      - Predicts binary logits on 'row' nodes.\n",
        "\n",
        "    Recommended:\n",
        "      - num_layers=2 for \"row connected to value with degree k\" style tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata: Tuple[List[str], List[Tuple[str, str, str]]],\n",
        "        row_node_type: str = \"row\",\n",
        "        hidden_channels: int = 64,\n",
        "        num_layers: int = 2,\n",
        "        dropout: float = 0.02,\n",
        "        sage_aggr: str = \"sum\",\n",
        "        init_noise_std: float = 1e-3,  # symmetry breaker\n",
        "        head_hidden: int = 64,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.node_types, self.edge_types = metadata\n",
        "        self.row_node_type = row_node_type\n",
        "        self.hidden = hidden_channels\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.init_noise_std = init_noise_std\n",
        "\n",
        "        # One learnable type vector per node type (broadcast across nodes of that type)\n",
        "        self.type_vec = nn.ParameterDict()\n",
        "        for ntype in self.node_types:\n",
        "            p = nn.Parameter(torch.zeros(hidden_channels))\n",
        "            nn.init.normal_(p, mean=0.0, std=0.02)\n",
        "            self.type_vec[ntype] = p\n",
        "\n",
        "        # Hetero GraphSAGE layers (SUM across relations)\n",
        "        self.convs = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.convs.append(\n",
        "                HeteroConv(\n",
        "                    {etype: SAGEConv((-1, -1), hidden_channels, aggr=sage_aggr) for etype in self.edge_types},\n",
        "                    aggr=\"sum\",\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Small head for binary logits\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden_channels, head_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(head_hidden, 1),\n",
        "        )\n",
        "\n",
        "    def _init_x_dict(self, data: HeteroData) -> Dict[str, torch.Tensor]:\n",
        "        device = next(self.parameters()).device\n",
        "        x_dict: Dict[str, torch.Tensor] = {}\n",
        "        for ntype in self.node_types:\n",
        "            n = int(data[ntype].num_nodes)\n",
        "            base = self.type_vec[ntype].to(device).unsqueeze(0).expand(n, -1)\n",
        "\n",
        "            # Add tiny noise to break symmetry (same distribution each forward pass)\n",
        "            if self.init_noise_std > 0:\n",
        "                base = base + self.init_noise_std * torch.randn((n, self.hidden), device=device)\n",
        "\n",
        "            x_dict[ntype] = base\n",
        "        return x_dict\n",
        "\n",
        "    def _edge_index_dict(self, data: HeteroData) -> Dict[Tuple[str, str, str], torch.Tensor]:\n",
        "        device = next(self.parameters()).device\n",
        "        out = {}\n",
        "        for etype in self.edge_types:\n",
        "            store = data[etype]\n",
        "            if \"edge_index\" in store and store[\"edge_index\"] is not None and store[\"edge_index\"].numel() > 0:\n",
        "                out[etype] = store[\"edge_index\"].to(device)\n",
        "        return out\n",
        "\n",
        "    def forward(self, data: HeteroData) -> torch.Tensor:\n",
        "        device = next(self.parameters()).device\n",
        "        data = data.to(device)\n",
        "\n",
        "        x_dict = self._init_x_dict(data)\n",
        "        edge_index_dict = self._edge_index_dict(data)\n",
        "\n",
        "        for li, conv in enumerate(self.convs):\n",
        "            x_dict = conv(x_dict, edge_index_dict)\n",
        "\n",
        "            if li < self.num_layers - 1:\n",
        "                for ntype in x_dict:\n",
        "                    x_dict[ntype] = F.relu(x_dict[ntype])\n",
        "                    x_dict[ntype] = F.dropout(x_dict[ntype], p=self.dropout, training=self.training)\n",
        "\n",
        "        logits = self.head(x_dict[self.row_node_type])\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "g4nUgpry9Tlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "metadata = (data_train.node_types, data_train.edge_types)\n",
        "\n",
        "model = StructuralInductiveHeteroSAGE(\n",
        "    metadata=metadata,\n",
        "    row_node_type=\"row\",\n",
        "    hidden_channels=64,\n",
        "    num_layers=4,\n",
        "    dropout=0.0,\n",
        "    sage_aggr=\"sum\",\n",
        "    init_noise_std=1e-3,\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "VR95SlLj--w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = data_train[\"row\"].y.cpu().numpy()\n",
        "pos = (y == 1).sum()\n",
        "neg = (y == 0).sum()\n",
        "pos_weight = neg / max(pos, 1)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss(\n",
        "    pos_weight=torch.tensor([pos_weight], device=device)\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(data_train.to(device)).view(-1)\n",
        "print(\"logits std:\", logits.std().item())\n"
      ],
      "metadata": {
        "id": "NfCsbaI--_Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _predict_probs(model: nn.Module, data, device: torch.device):\n",
        "    model.eval()\n",
        "    d = data.to(device)\n",
        "    logits = model(d).view(-1)\n",
        "    probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "    y = d[\"row\"].y.view(-1).detach().cpu().numpy().astype(np.int32)\n",
        "    return probs, y\n",
        "\n",
        "\n",
        "def _roc_auc_safe(y: np.ndarray, probs: np.ndarray) -> float:\n",
        "    if len(np.unique(y)) < 2:\n",
        "        return float(\"nan\")\n",
        "    return float(roc_auc_score(y, probs))\n",
        "\n",
        "\n",
        "def _best_f1_threshold(y: np.ndarray, probs: np.ndarray, grid: int = 200) -> tuple[float, float]:\n",
        "    # threshold grid in (0,1)\n",
        "    ts = np.linspace(0.001, 0.999, grid)\n",
        "    best_t, best_f1 = 0.5, -1.0\n",
        "    for t in ts:\n",
        "        f1 = f1_score(y, (probs >= t).astype(np.int32))\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_t = float(f1), float(t)\n",
        "    return best_t, best_f1\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, data, device: torch.device, threshold: float) -> dict:\n",
        "    probs, y = _predict_probs(model, data, device)\n",
        "    y_pred = (probs >= threshold).astype(np.int32)\n",
        "    return {\n",
        "        \"f1\": float(f1_score(y, y_pred)),\n",
        "        \"roc_auc\": _roc_auc_safe(y, probs),\n",
        "    }\n",
        "\n",
        "\n",
        "def train_binary_best_f1(\n",
        "    model: nn.Module,\n",
        "    data_train,\n",
        "    data_val,\n",
        "    data_test,\n",
        "    *,\n",
        "    device: torch.device,\n",
        "    lr: float = 1e-3,\n",
        "    weight_decay: float = 1e-4,\n",
        "    epochs: int = 50,\n",
        "    grad_clip: float | None = 1.0,\n",
        "    threshold_grid: int = 200,\n",
        "):\n",
        "    model = model.to(device)\n",
        "\n",
        "    # pos_weight from TRAIN\n",
        "    y_train_np = data_train[\"row\"].y.detach().cpu().numpy()\n",
        "    pos = float((y_train_np == 1).sum())\n",
        "    neg = float((y_train_np == 0).sum())\n",
        "    pos_weight = neg / max(pos, 1.0)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    best = {\n",
        "        \"val_f1\": -1.0,\n",
        "        \"threshold\": 0.5,\n",
        "        \"state_dict\": None,\n",
        "        \"epoch\": -1,\n",
        "        \"val_auc\": float(\"nan\"),\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        dtr = data_train.to(device)\n",
        "        logits = model(dtr).view(-1)\n",
        "        y = dtr[\"row\"].y.view(-1).float()\n",
        "\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "\n",
        "        if grad_clip is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Pick threshold on VAL to maximize F1\n",
        "        val_probs, val_y = _predict_probs(model, data_val, device)\n",
        "        t_star, val_f1 = _best_f1_threshold(val_y, val_probs, grid=threshold_grid)\n",
        "        val_auc = _roc_auc_safe(val_y, val_probs)\n",
        "\n",
        "        # Compute train metrics at the same threshold (for interpretability)\n",
        "        train_metrics = evaluate(model, data_train, device, threshold=t_star)\n",
        "        val_metrics = {\"f1\": float(val_f1), \"roc_auc\": float(val_auc)}\n",
        "\n",
        "        # Checkpoint by val F1\n",
        "        if val_f1 > best[\"val_f1\"]:\n",
        "            best[\"val_f1\"] = float(val_f1)\n",
        "            best[\"val_auc\"] = float(val_auc)\n",
        "            best[\"threshold\"] = float(t_star)\n",
        "            best[\"epoch\"] = epoch\n",
        "            best[\"state_dict\"] = copy.deepcopy({k: v.detach().cpu() for k, v in model.state_dict().items()})\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | loss={loss.item():.4f} | \"\n",
        "            f\"thr*={t_star:.3f} | \"\n",
        "            f\"train_f1={train_metrics['f1']:.4f} train_auc={train_metrics['roc_auc']:.4f} | \"\n",
        "            f\"val_f1={val_metrics['f1']:.4f} val_auc={val_metrics['roc_auc']:.4f}\"\n",
        "        )\n",
        "\n",
        "    # Restore best model\n",
        "    if best[\"state_dict\"] is not None:\n",
        "        model.load_state_dict(best[\"state_dict\"])\n",
        "\n",
        "    # Final report on TEST using best threshold selected on VAL\n",
        "    test_metrics = evaluate(model, data_test, device, threshold=best[\"threshold\"])\n",
        "\n",
        "    report = {\n",
        "        \"best_epoch\": best[\"epoch\"],\n",
        "        \"best_val_f1\": best[\"val_f1\"],\n",
        "        \"best_val_auc\": best[\"val_auc\"],\n",
        "        \"best_threshold\": best[\"threshold\"],\n",
        "        \"test_f1\": test_metrics[\"f1\"],\n",
        "        \"test_auc\": test_metrics[\"roc_auc\"],\n",
        "        \"pos_weight\": float(pos_weight),\n",
        "    }\n",
        "    return model, report\n"
      ],
      "metadata": {
        "id": "lkb8oKNa_EcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(task_col)\n",
        "\n",
        "model, report = train_binary_best_f1(\n",
        "    model,\n",
        "    data_train,\n",
        "    data_val,\n",
        "    data_test,\n",
        "    device=device,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    epochs=100,\n",
        ")\n",
        "\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "zMrTLGKl_ULO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pOXy_3HD5yMf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}