{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "\n",
        "\n",
        "!pip install -q --no-cache-dir \\\n",
        "torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 \\\n",
        "--index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laFayG_CiGsJ",
        "outputId": "8898fdba-0644-4ca3-d6a6-0bfd35fa389e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.9.0+cu126\n",
            "Uninstalling torch-2.9.0+cu126:\n",
            "  Successfully uninstalled torch-2.9.0+cu126\n",
            "Found existing installation: torchvision 0.24.0+cu126\n",
            "Uninstalling torchvision-0.24.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.24.0+cu126\n",
            "Found existing installation: torchaudio 2.9.0+cu126\n",
            "Uninstalling torchaudio-2.9.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.9.0+cu126\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 MB\u001b[0m \u001b[31m260.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m221.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m153.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m314.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m410.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m347.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m235.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m237.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m238.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m270.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m199.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m271.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m321.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m241.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m321.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --no-cache-dir \\\n",
        "  pyg-lib torch-scatter torch-sparse torch-cluster torch-spline-conv \\\n",
        "  -f https://data.pyg.org/whl/torch-2.4.1+cu121.html\n",
        "\n",
        "!pip install -q --no-cache-dir torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlHCRPDEkgSU",
        "outputId": "dd930443-4b54-4808-f4f4-5bd554980d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m989.6/989.6 kB\u001b[0m \u001b[31m233.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install relbench"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCP1a_JMU2Kk",
        "outputId": "f726a0b9-5481-4b2d-aac2-0ab15efc81d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting relbench\n",
            "  Downloading relbench-2.0.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from relbench) (2.2.2)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.12/dist-packages (from relbench) (1.8.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from relbench) (18.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from relbench) (2.0.2)\n",
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.12/dist-packages (from relbench) (1.3.2)\n",
            "Requirement already satisfied: scikit-learn<=1.6.1 in /usr/local/lib/python3.12/dist-packages (from relbench) (1.6.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from relbench) (4.15.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from relbench) (4.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<=1.6.1->relbench) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<=1.6.1->relbench) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<=1.6.1->relbench) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets->relbench) (3.20.3)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->relbench) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets->relbench) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets->relbench) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->relbench) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->relbench) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->relbench) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets->relbench) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets->relbench) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets->relbench) (6.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->relbench) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->relbench) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->relbench) (2025.3)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch->relbench) (4.5.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->relbench) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets->relbench) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->relbench) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->relbench) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->relbench) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->relbench) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->relbench) (2026.1.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->relbench) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->relbench) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->relbench) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->relbench) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->relbench) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->relbench) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->relbench) (1.22.0)\n",
            "Downloading relbench-2.0.2-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: relbench\n",
            "Successfully installed relbench-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U \"autogluon.tabular[all]\""
      ],
      "metadata": {
        "id": "9dFq8Y8GRsSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"pyg:\", torch_geometric.__version__)\n",
        "print(\"NeighborLoader import OK\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1DK4kpikimQ",
        "outputId": "681883ac-ae03-44a0-830c-a72f1b438786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.4.1+cu121\n",
            "pyg: 2.7.0\n",
            "NeighborLoader import OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "SqgzSt0cWkHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from relbench.datasets import get_dataset\n",
        "\n",
        "dataset = get_dataset(name=\"rel-trial\", download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uslZtXsU9Lv",
        "outputId": "8c04c0ed-41e0-48a6-b2bd-6ed849fb070a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading file 'rel-trial/db.zip' from 'https://relbench.stanford.edu/download/rel-trial/db.zip' to '/root/.cache/relbench'.\n",
            "100%|████████████████████████████████████████| 574M/574M [00:00<00:00, 970GB/s]\n",
            "Unzipping contents of '/root/.cache/relbench/rel-trial/db.zip' to '/root/.cache/relbench/rel-trial/.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from relbench.tasks import get_task\n",
        "\n",
        "db = dataset.get_db()\n",
        "\n",
        "table = db.table_dict[\"studies\"]\n",
        "task = get_task(\"rel-trial\", \"study-outcome\", download=True)\n",
        "\n",
        "train_task = task.get_table(\"train\").df\n",
        "val_task = task.get_table(\"val\").df\n",
        "test_task = task.get_table(\"test\").df\n",
        "\n",
        "## learning table\n",
        "\n",
        "df_train = train_task.merge(table.df, how=\"left\")\n",
        "df_val = val_task.merge(table.df, how=\"left\")\n",
        "df_test = test_task.merge(table.df, how=\"left\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PdfRrI_U963",
        "outputId": "28ea3312-747d-4e98-ffd9-9f8aaee8fb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Database object from /root/.cache/relbench/rel-trial/db...\n",
            "Done in 16.05 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading file 'rel-trial/tasks/study-outcome.zip' from 'https://relbench.stanford.edu/download/rel-trial/tasks/study-outcome.zip' to '/root/.cache/relbench'.\n",
            "100%|█████████████████████████████████████| 78.9k/78.9k [00:00<00:00, 18.6MB/s]\n",
            "Unzipping contents of '/root/.cache/relbench/rel-trial/tasks/study-outcome.zip' to '/root/.cache/relbench/rel-trial/tasks/.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the graph"
      ],
      "metadata": {
        "id": "lZ2ZFH0ZKr85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Collection, Dict, List, Optional, Tuple, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "def df_to_hetero_for_task(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    features: List[str],\n",
        "    task_col: Optional[str] = None,     # <-- change\n",
        "    row_encoder=None,\n",
        "    row_node_type: str = \"row\",\n",
        "    feature_node_prefix: str = \"\",\n",
        "    include_missing: bool = True,\n",
        "    missing_token: str = \"__MISSING__\",\n",
        "    oov_token: str = \"__OOV__\",\n",
        "    add_reverse_edges: bool = True,\n",
        "    store_value_strings: bool = False,\n",
        "    x_dtype: torch.dtype = torch.float32,\n",
        "    y_dtype: torch.dtype = torch.float32,\n",
        "    value2idx_in: Optional[Dict[str, Dict[Any, int]]] = None,\n",
        "    use_one_hot: Optional[Union[bool, Collection[str]]] = None,\n",
        "    one_hot_max_size: int = 5000,\n",
        ") -> Tuple[HeteroData, Dict[str, Dict[Any, int]]]:\n",
        "\n",
        "    # If task_col is provided, require it. If not, build unlabeled graph.\n",
        "    if task_col is not None and task_col not in df.columns:\n",
        "        raise KeyError(f\"task_col='{task_col}' not in df.columns.\")\n",
        "\n",
        "    for f in features:\n",
        "        if f not in df.columns:\n",
        "            raise KeyError(f\"Feature '{f}' not in df.columns.\")\n",
        "\n",
        "    if use_one_hot is True:\n",
        "        one_hot_feats = set(features)\n",
        "    elif use_one_hot:\n",
        "        one_hot_feats = set(use_one_hot)\n",
        "    else:\n",
        "        one_hot_feats = set()\n",
        "\n",
        "    data = HeteroData()\n",
        "    n = len(df)\n",
        "\n",
        "    data[row_node_type].num_nodes = n\n",
        "\n",
        "    # Only attach labels when available\n",
        "    if task_col is not None:\n",
        "        data[row_node_type].y = torch.tensor(\n",
        "            df[task_col].to_numpy(), dtype=y_dtype\n",
        "        ).view(-1)\n",
        "\n",
        "    if row_encoder is not None:\n",
        "        X, _names = row_encoder.transform(df)\n",
        "        data[row_node_type].x = torch.tensor(X, dtype=x_dtype)\n",
        "\n",
        "    row_idx = np.arange(n, dtype=np.int64)\n",
        "\n",
        "    value2idx: Dict[str, Dict[Any, int]] = {} if value2idx_in is None else value2idx_in\n",
        "\n",
        "    for feat in features:\n",
        "        feat_type = f\"{feature_node_prefix}{feat}\"\n",
        "        rel_type = f\"has_{feat}\"\n",
        "        rev_rel_type = f\"rev_{rel_type}\"\n",
        "\n",
        "        col = df[feat]\n",
        "        if include_missing:\n",
        "            col = col.where(~col.isna(), other=missing_token)\n",
        "\n",
        "        if value2idx_in is None:\n",
        "            uniques = list(pd.unique(col if include_missing else df[feat].dropna()))\n",
        "            if include_missing and missing_token not in uniques:\n",
        "                uniques.append(missing_token)\n",
        "            if oov_token not in uniques:\n",
        "                uniques.append(oov_token)\n",
        "            v2i = {v: i for i, v in enumerate(uniques)}\n",
        "            value2idx[feat] = v2i\n",
        "        else:\n",
        "            v2i = value2idx_in[feat]\n",
        "            if oov_token not in v2i:\n",
        "                raise ValueError(f\"oov_token='{oov_token}' missing in value2idx_in[{feat!r}]\")\n",
        "\n",
        "        num_vals = len(v2i)\n",
        "        data[feat_type].num_nodes = num_vals\n",
        "\n",
        "        if (feat in one_hot_feats) and (num_vals <= one_hot_max_size):\n",
        "            data[feat_type].x = torch.eye(num_vals, dtype=x_dtype)\n",
        "\n",
        "        if store_value_strings and value2idx_in is None:\n",
        "            data[feat_type].value = list(v2i.keys())\n",
        "\n",
        "        oov_idx = v2i[oov_token]\n",
        "        vals = col.to_numpy()\n",
        "        dst = np.fromiter((v2i.get(v, oov_idx) for v in vals), dtype=np.int64, count=len(vals))\n",
        "\n",
        "        edge_index = torch.tensor(np.vstack([row_idx, dst]), dtype=torch.long)\n",
        "        data[(row_node_type, rel_type, feat_type)].edge_index = edge_index\n",
        "\n",
        "        if add_reverse_edges:\n",
        "            data[(feat_type, rev_rel_type, row_node_type)].edge_index = edge_index.flip(0).contiguous()\n",
        "\n",
        "    return data, value2idx\n"
      ],
      "metadata": {
        "id": "DoB15gIz4psB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = ['outcome']\n",
        "\n",
        "features = [\n",
        "        \"fdaaa801_violation\",\n",
        "        \"is_unapproved_device\",\n",
        "        \"is_ppsd\",\n",
        "        \"is_fda_regulated_device\",\n",
        "        \"enrollment_type\",\n",
        "        \"has_dmc\",\n",
        "        \"is_us_export\",\n",
        "        \"is_fda_regulated_drug\",\n",
        "        \"plan_to_share_ipd\",\n",
        "        \"biospec_retention\",\n",
        "        \"study_type\",\n",
        "        \"source_class\",\n",
        "        \"phase\",\n",
        "    ] # features to encode as nodes\n",
        "\n",
        "task_col = \"outcome\"\n",
        "data_train, v2i_train = df_to_hetero_for_task(\n",
        "    df_train,\n",
        "    features=features,\n",
        "    task_col=task_col,\n",
        "    row_encoder=None,          # we will attach row.x ourselves\n",
        "    add_reverse_edges=True,\n",
        "    include_missing=True,\n",
        "    missing_token=\"__MISSING__\",\n",
        "    oov_token=\"__OOV__\",       # useful for missing values\n",
        "    value2idx_in=None,         # IMPORTANT: per-split vocab\n",
        "    use_one_hot=None,\n",
        ")\n",
        "\n",
        "data_val, v2i_val = df_to_hetero_for_task(\n",
        "    df_val,\n",
        "    features=features,\n",
        "    task_col=task_col,\n",
        "    row_encoder=None,\n",
        "    add_reverse_edges=True,\n",
        "    include_missing=True,\n",
        "    missing_token=\"__MISSING__\",\n",
        "    oov_token=\"__OOV__\",\n",
        "    value2idx_in=None,         # IMPORTANT: per-split vocab\n",
        "    use_one_hot=None,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "data_test, v2i_test = df_to_hetero_for_task(\n",
        "    df_test,\n",
        "    features=features,\n",
        "    row_encoder=None,\n",
        "    add_reverse_edges=True,\n",
        "    include_missing=True,\n",
        "    missing_token=\"__MISSING__\",\n",
        "    oov_token=\"__OOV__\",\n",
        "    value2idx_in=None,         # IMPORTANT: per-split vocab\n",
        "    use_one_hot=None,\n",
        ")\n"
      ],
      "metadata": {
        "id": "X9yScVoxgw8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess node level row fearures"
      ],
      "metadata": {
        "id": "It58GZMLijMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional, Union\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "\n",
        "class RowFeaturePreprocessor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        text_cols: Optional[Union[str, List[str]]] = None,\n",
        "        numeric_cols: List[str],\n",
        "        categorical_cols: Optional[List[str]] = None,\n",
        "        max_tfidf_features: int = 128,\n",
        "        min_df: int = 2,\n",
        "        lowercase: bool = True,\n",
        "        cat_missing_token: str = \"__MISSING__\",\n",
        "    ):\n",
        "        # normalize text_cols\n",
        "        if isinstance(text_cols, str):\n",
        "            text_cols = [text_cols]\n",
        "\n",
        "        self.text_cols = text_cols or []\n",
        "        self.numeric_cols = numeric_cols\n",
        "        self.categorical_cols = categorical_cols or []\n",
        "\n",
        "        self.max_tfidf_features = max_tfidf_features\n",
        "        self.min_df = min_df\n",
        "        self.lowercase = lowercase\n",
        "        self.cat_missing_token = cat_missing_token\n",
        "\n",
        "        self._tfidf = None\n",
        "        self._scaler = None\n",
        "        self._ohe = None\n",
        "        self._fitted = False\n",
        "\n",
        "    def fit(self, df: pd.DataFrame):\n",
        "        # ---- numeric ----\n",
        "        if self.numeric_cols:\n",
        "            X_num = (\n",
        "                df[self.numeric_cols]\n",
        "                .apply(pd.to_numeric, errors=\"coerce\")\n",
        "                .fillna(0.0)\n",
        "                .to_numpy(dtype=np.float32)\n",
        "            )\n",
        "            self._scaler = StandardScaler()\n",
        "            self._scaler.fit(X_num)\n",
        "\n",
        "        # ---- categorical ----\n",
        "        if self.categorical_cols:\n",
        "            X_cat = (\n",
        "                df[self.categorical_cols]\n",
        "                .astype(\"object\")\n",
        "                .where(~df[self.categorical_cols].isna(), self.cat_missing_token)\n",
        "            )\n",
        "            self._ohe = OneHotEncoder(\n",
        "                handle_unknown=\"ignore\",\n",
        "                sparse_output=False,\n",
        "                dtype=np.float32,\n",
        "            )\n",
        "            self._ohe.fit(X_cat)\n",
        "\n",
        "        # ---- text (single TF-IDF over concatenated columns) ----\n",
        "        if self.text_cols:\n",
        "            text = self._combine_text_cols(df)\n",
        "            self._tfidf = TfidfVectorizer(\n",
        "                max_features=self.max_tfidf_features,\n",
        "                min_df=self.min_df,\n",
        "                lowercase=self.lowercase,\n",
        "                ngram_range=(1, 2),\n",
        "            )\n",
        "            self._tfidf.fit(text)\n",
        "\n",
        "        self._fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, df: pd.DataFrame) -> np.ndarray:\n",
        "        if not self._fitted:\n",
        "            raise RuntimeError(\"RowFeaturePreprocessor must be fitted first\")\n",
        "\n",
        "        parts = []\n",
        "\n",
        "        # numeric\n",
        "        if self.numeric_cols:\n",
        "            X_num = (\n",
        "                df[self.numeric_cols]\n",
        "                .apply(pd.to_numeric, errors=\"coerce\")\n",
        "                .fillna(0.0)\n",
        "                .to_numpy(dtype=np.float32)\n",
        "            )\n",
        "            X_num = self._scaler.transform(X_num)\n",
        "            parts.append(X_num)\n",
        "\n",
        "        # categorical\n",
        "        if self.categorical_cols:\n",
        "            X_cat = (\n",
        "                df[self.categorical_cols]\n",
        "                .astype(\"object\")\n",
        "                .where(~df[self.categorical_cols].isna(), self.cat_missing_token)\n",
        "            )\n",
        "            X_cat = self._ohe.transform(X_cat)\n",
        "            parts.append(X_cat)\n",
        "\n",
        "        # text\n",
        "        if self.text_cols:\n",
        "            text = self._combine_text_cols(df)\n",
        "            X_text = self._tfidf.transform(text).toarray().astype(np.float32)\n",
        "            parts.append(X_text)\n",
        "\n",
        "        if not parts:\n",
        "            return np.zeros((len(df), 0), dtype=np.float32)\n",
        "\n",
        "        return np.hstack(parts)\n",
        "\n",
        "    def _combine_text_cols(self, df: pd.DataFrame) -> pd.Series:\n",
        "        # Safe concatenation with missing handling\n",
        "        s = (\n",
        "            df[self.text_cols]\n",
        "            .fillna(\"\")\n",
        "            .astype(str)\n",
        "        )\n",
        "        return s.apply(lambda r: \" \".join(v for v in r.values if v), axis=1)\n",
        "\n",
        "    @property\n",
        "    def output_dim(self) -> int:\n",
        "        if not self._fitted:\n",
        "            raise RuntimeError(\"Not fitted\")\n",
        "\n",
        "        dim = 0\n",
        "        if self.numeric_cols:\n",
        "            dim += len(self.numeric_cols)\n",
        "        if self.categorical_cols:\n",
        "            dim += sum(len(cats) for cats in self._ohe.categories_)\n",
        "        if self.text_cols:\n",
        "            dim += len(self._tfidf.get_feature_names_out())\n",
        "        return dim"
      ],
      "metadata": {
        "id": "cgMvprjkiRKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUMERIC_COLS = [\n",
        "\"target_duration\",\n",
        "\"enrollment\",\n",
        "\"number_of_arms\",\n",
        "\"number_of_groups\",\n",
        "]\n",
        "\n",
        "CATEGORICAL_COLS = [\n",
        "\"study_type\",\n",
        "\"phase\",\n",
        "\"enrollment_type\",\n",
        "\"has_dmc\",\n",
        "\"is_fda_regulated_drug\",\n",
        "\"is_fda_regulated_device\",\n",
        "\"is_unapproved_device\",\n",
        "\"is_us_export\",\n",
        "\"biospec_retention\",\n",
        "\"source_class\",\n",
        "\"fdaaa801_violation\",\n",
        "\"plan_to_share_ipd\",\n",
        "]\n",
        "\n",
        "TEXT_COLS = [\n",
        "\"acronym\",\n",
        "\"baseline_population\",\n",
        "\"brief_title\",\n",
        "\"official_title\",\n",
        "\"source\",\n",
        "\"is_ppsd\",\n",
        "\"biospec_description\",\n",
        "\"baseline_type_units_analyzed\",\n",
        "\"detailed_descriptions\",\n",
        "\"brief_summaries\",\n",
        "]\n",
        "\n",
        "\n",
        "pp = RowFeaturePreprocessor(\n",
        "    text_cols=TEXT_COLS,\n",
        "    numeric_cols=NUMERIC_COLS,\n",
        "    categorical_cols=CATEGORICAL_COLS,\n",
        "    max_tfidf_features=256,   # hyperparameter, maybe change?\n",
        ").fit(df_train)\n",
        "\n",
        "data_train[\"row\"].x = torch.tensor(pp.transform(df_train), dtype=torch.float32)\n",
        "data_val[\"row\"].x   = torch.tensor(pp.transform(df_val),   dtype=torch.float32)\n",
        "data_test[\"row\"].x  = torch.tensor(pp.transform(df_test),  dtype=torch.float32)\n",
        "\n",
        "print(\"row.x dim:\", data_train[\"row\"].x.shape[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlQqaGsWhXB7",
        "outputId": "dd405237-da69-48e1-9434-f8154691fc59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "row.x dim: 305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNN"
      ],
      "metadata": {
        "id": "zpFx7HvDpSZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import HeteroConv, SAGEConv\n",
        "\n",
        "\n",
        "class WideDeepHeteroSAGE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata: Tuple[List[str], List[Tuple[str, str, str]]],\n",
        "        *,\n",
        "        num_nodes_by_type: Dict[str, int],\n",
        "        row_in_dim: int,\n",
        "        row_node_type: str = \"row\",\n",
        "        hidden_channels: int = 128,\n",
        "        num_layers: int = 1,\n",
        "        dropout: float = 0.2,\n",
        "        sage_aggr: str = \"mean\",\n",
        "        mlp_hidden: int = 256,\n",
        "        use_layernorm: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.node_types, self.edge_types = metadata\n",
        "        self.row_node_type = row_node_type\n",
        "        self.hidden = hidden_channels\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # --- Wide path: row-only MLP ---\n",
        "        self.row_mlp = nn.Sequential(\n",
        "            nn.Linear(row_in_dim, mlp_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden, hidden_channels),\n",
        "        )\n",
        "\n",
        "        # --- Graph path init for row nodes (project row.x) ---\n",
        "        self.row_proj = nn.Linear(row_in_dim, hidden_channels)\n",
        "\n",
        "        # --- Value-node embeddings ---\n",
        "        self.emb = nn.ModuleDict()\n",
        "        for ntype in self.node_types:\n",
        "            if ntype == row_node_type:\n",
        "                continue\n",
        "            self.emb[ntype] = nn.Embedding(int(num_nodes_by_type[ntype]), hidden_channels)\n",
        "\n",
        "        # --- Hetero GraphSAGE ---\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.norms = nn.ModuleList() if use_layernorm else None\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            self.convs.append(\n",
        "                HeteroConv(\n",
        "                    {etype: SAGEConv((-1, -1), hidden_channels, aggr=sage_aggr) for etype in self.edge_types},\n",
        "                    aggr=\"sum\",\n",
        "                )\n",
        "            )\n",
        "            if use_layernorm:\n",
        "                self.norms.append(nn.ModuleDict({nt: nn.LayerNorm(hidden_channels) for nt in self.node_types}))\n",
        "\n",
        "        # --- Fusion gate: decides how much graph vs wide signal ---\n",
        "        # gate in (0,1): fused = wide + gate * graph\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_channels, hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_channels, 1),\n",
        "        )\n",
        "\n",
        "\n",
        "        # make initial gate ~ small (sigmoid(-2) ~ 0.12)\n",
        "        nn.init.constant_(self.gate[-1].bias, -2.0)\n",
        "\n",
        "        # --- Head ---\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden_channels, hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_channels, 1),\n",
        "        )\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "        for e in self.emb.values():\n",
        "            nn.init.normal_(e.weight, mean=0.0, std=0.02)\n",
        "        for conv in self.convs:\n",
        "            for c in conv.convs.values():\n",
        "                c.reset_parameters()\n",
        "\n",
        "    def _init_x_dict(self, data: HeteroData) -> Dict[str, torch.Tensor]:\n",
        "        device = next(self.parameters()).device\n",
        "        x_dict: Dict[str, torch.Tensor] = {}\n",
        "\n",
        "        row_x = data[self.row_node_type].x.to(device)\n",
        "        x_dict[self.row_node_type] = self.row_proj(row_x)\n",
        "\n",
        "        for ntype in self.node_types:\n",
        "            if ntype == self.row_node_type:\n",
        "                continue\n",
        "            n = int(data[ntype].num_nodes)\n",
        "            if hasattr(data[ntype], \"n_id\") and data[ntype].n_id is not None:\n",
        "                idx = data[ntype].n_id.to(device)  # global ids\n",
        "            else:\n",
        "                idx = torch.arange(n, device=device)\n",
        "            x_dict[ntype] = self.emb[ntype](idx)\n",
        "\n",
        "        return x_dict\n",
        "\n",
        "    def forward(self, data: HeteroData) -> torch.Tensor:\n",
        "        device = next(self.parameters()).device\n",
        "        data = data.to(device)\n",
        "\n",
        "        # Wide path representation (row-only)\n",
        "        row_x = data[self.row_node_type].x\n",
        "        h_wide = self.row_mlp(row_x)\n",
        "\n",
        "        # Graph path representation\n",
        "        x_dict = self._init_x_dict(data)\n",
        "        edge_index_dict = data.edge_index_dict\n",
        "\n",
        "        for li, conv in enumerate(self.convs):\n",
        "            x_dict = conv(x_dict, edge_index_dict)\n",
        "            if li < self.num_layers - 1:\n",
        "                for ntype in x_dict:\n",
        "                    if self.norms is not None:\n",
        "                        x_dict[ntype] = self.norms[li][ntype](x_dict[ntype])\n",
        "                    x_dict[ntype] = F.relu(x_dict[ntype])\n",
        "                    x_dict[ntype] = F.dropout(x_dict[ntype], p=self.dropout, training=self.training)\n",
        "\n",
        "        h_graph = x_dict[self.row_node_type]\n",
        "\n",
        "        # Fuse (wide + gated graph)\n",
        "        gate = torch.sigmoid(self.gate(torch.cat([h_wide, h_graph], dim=-1)))  # [N,1]\n",
        "        h = h_wide + gate * h_graph\n",
        "\n",
        "        logits = self.head(h).view(-1)\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def embed_rows(self, data, device, which: str = \"fused\"):\n",
        "        \"\"\"\n",
        "        Returns row embeddings aligned to row order in `data`.\n",
        "        which in {\"wide\", \"graph\", \"fused\"}.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        d = data.to(device)\n",
        "\n",
        "        row_x = d[\"row\"].x\n",
        "        h_wide = self.row_mlp(row_x)\n",
        "\n",
        "        x_dict = self._init_x_dict(d)\n",
        "        edge_index_dict = d.edge_index_dict\n",
        "        for li, conv in enumerate(self.convs):\n",
        "            x_dict = conv(x_dict, edge_index_dict)\n",
        "            if li < self.num_layers - 1:\n",
        "                for ntype in x_dict:\n",
        "                    if self.norms is not None:\n",
        "                        x_dict[ntype] = self.norms[li][ntype]\n",
        "                    x_dict[ntype] = F.relu(x_dict[ntype])\n",
        "                    x_dict[ntype] = F.dropout(x_dict[ntype], p=self.dropout, training=False)\n",
        "\n",
        "        h_graph = x_dict[\"row\"]\n",
        "        gate = torch.sigmoid(self.gate(torch.cat([h_wide, h_graph], dim=-1)))\n",
        "        h_fused = h_wide + gate * h_graph\n",
        "\n",
        "        if which == \"wide\":\n",
        "            return h_wide.detach().cpu()\n",
        "        if which == \"graph\":\n",
        "            return h_graph.detach().cpu()\n",
        "        if which == \"fused\":\n",
        "            return h_fused.detach().cpu()\n",
        "        raise ValueError(\"which must be one of: wide, graph, fused\")"
      ],
      "metadata": {
        "id": "K5JdzO1KpTCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "\n",
        "def make_train_loader(\n",
        "    data_train,\n",
        "    *,\n",
        "    batch_size: int = 1024,\n",
        "    num_neighbors=(25,),   # start 1-hop; later try (25,10)\n",
        "    num_workers: int = 0,\n",
        "):\n",
        "    train_idx = torch.arange(data_train[\"row\"].num_nodes)\n",
        "    return NeighborLoader(\n",
        "        data_train,\n",
        "        input_nodes=(\"row\", train_idx),\n",
        "        num_neighbors=list(num_neighbors),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "    )"
      ],
      "metadata": {
        "id": "qEgQc_JGpTeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "def roc_auc_safe(y: np.ndarray, p: np.ndarray) -> float:\n",
        "    return float(\"nan\") if len(np.unique(y)) < 2 else float(roc_auc_score(y, p))\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_fullbatch_probs(model, data, device):\n",
        "    model.eval()\n",
        "    d = data.to(device)\n",
        "    logits = model(d).view(-1)\n",
        "    probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "    y = d[\"row\"].y.view(-1).detach().cpu().numpy().astype(np.int32)\n",
        "    return probs, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_fullbatch_auc(model, data, device):\n",
        "    p, y = eval_fullbatch_probs(model, data, device)\n",
        "    return roc_auc_safe(y, p)\n",
        "\n",
        "def best_f1_threshold(y: np.ndarray, p: np.ndarray, grid: int = 200):\n",
        "    ts = np.linspace(0.01, 0.99, grid)\n",
        "    best_t, best_f1 = 0.5, -1.0\n",
        "    for t in ts:\n",
        "        f1 = f1_score(y, (p >= t).astype(np.int32))\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_t = float(f1), float(t)\n",
        "    return best_t, best_f1"
      ],
      "metadata": {
        "id": "qUSSc_zxpm1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_widedeep_neighbors_best_auc(\n",
        "    model,\n",
        "    *,\n",
        "    data_train,\n",
        "    data_val,\n",
        "    train_loader,\n",
        "    device,\n",
        "    lr: float = 3e-4,\n",
        "    weight_decay: float = 1e-4,\n",
        "    epochs: int = 50,\n",
        "    patience: int = 10,\n",
        "    grad_clip: float | None = 1.0,\n",
        "    print_every: int = 1,\n",
        "):\n",
        "    model = model.to(device)\n",
        "\n",
        "    # pos_weight from full train labels (recommended)\n",
        "    y_train = data_train[\"row\"].y.detach().cpu().numpy().astype(np.int32)\n",
        "    pos = float((y_train == 1).sum())\n",
        "    neg = float((y_train == 0).sum())\n",
        "    pos_weight = neg / max(pos, 1.0)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode=\"max\",\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    threshold=1e-4,\n",
        "    min_lr=1e-6,\n",
        "    verbose=True,)\n",
        "\n",
        "    best = {\"val_auc\": -1.0, \"epoch\": -1, \"state\": None}\n",
        "    bad = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss, total_seeds = 0.0, 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(batch).view(-1)\n",
        "            bs = int(batch[\"row\"].batch_size)  # seed rows\n",
        "            logits_seed = logits[:bs]\n",
        "            y_seed = batch[\"row\"].y[:bs].view(-1).float()\n",
        "\n",
        "            loss = criterion(logits_seed, y_seed)\n",
        "            loss.backward()\n",
        "\n",
        "            if grad_clip is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += float(loss.item()) * bs\n",
        "            total_seeds += bs\n",
        "\n",
        "        avg_loss = total_loss / max(total_seeds, 1)\n",
        "\n",
        "        # Full-batch validation AUC (stable)\n",
        "        val_auc = eval_fullbatch_auc(model, data_val, device)\n",
        "\n",
        "        improved = val_auc > best[\"val_auc\"] + 1e-6\n",
        "        if improved:\n",
        "            best[\"val_auc\"] = float(val_auc)\n",
        "            best[\"epoch\"] = epoch\n",
        "            best[\"state\"] = copy.deepcopy({k: v.detach().cpu() for k, v in model.state_dict().items()})\n",
        "            bad = 0\n",
        "        else:\n",
        "            bad += 1\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print(f\"Epoch {epoch:03d} | loss={avg_loss:.4f} | val_auc={val_auc:.4f} | bad={bad}/{patience}\")\n",
        "\n",
        "        if bad >= patience:\n",
        "            break\n",
        "\n",
        "    if best[\"state\"] is not None:\n",
        "        model.load_state_dict(best[\"state\"])\n",
        "\n",
        "    # Report best checkpoint + compute best-F1 threshold once\n",
        "    val_probs, val_y = eval_fullbatch_probs(model, data_val, device)\n",
        "    thr_star, val_f1 = best_f1_threshold(val_y, val_probs, grid=200)\n",
        "\n",
        "    report = {\n",
        "        \"best_epoch\": best[\"epoch\"],\n",
        "        \"best_val_auc\": best[\"val_auc\"],\n",
        "        \"val_f1_at_thr_star\": float(val_f1),\n",
        "        \"thr_star_from_val\": float(thr_star),\n",
        "        \"pos_weight\": float(pos_weight),\n",
        "    }\n",
        "    return model, report"
      ],
      "metadata": {
        "id": "UgXxuyAWpoTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "metadata = data_train.metadata()\n",
        "num_nodes_by_type = {nt: int(data_train[nt].num_nodes) for nt in metadata[0] if nt != \"row\"}\n",
        "\n",
        "model = WideDeepHeteroSAGE(\n",
        "    metadata=metadata,\n",
        "    num_nodes_by_type=num_nodes_by_type,\n",
        "    row_in_dim=305,          # your row.x dim\n",
        "    hidden_channels=64,\n",
        "    num_layers=1,            # start 1-hop\n",
        "    dropout=0.3,\n",
        "    sage_aggr=\"mean\",\n",
        "    mlp_hidden=256,\n",
        ")"
      ],
      "metadata": {
        "id": "Exqeizj9ppvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = make_train_loader(\n",
        "    data_train,\n",
        "    batch_size=1024,\n",
        "    num_neighbors=(25,),     # 1-hop\n",
        "    num_workers=0,\n",
        ")"
      ],
      "metadata": {
        "id": "JvV9RA8-prUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, report = train_widedeep_neighbors_best_auc(\n",
        "    model,\n",
        "    data_train=data_train,\n",
        "    data_val=data_val,\n",
        "    train_loader=train_loader,\n",
        "    device=device,\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4,\n",
        "    epochs=50,\n",
        "    patience=10,\n",
        ")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coNHf4M1puEG",
        "outputId": "189e536a-e23a-4185-94d3-d73286ee4b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | loss=0.4991 | val_auc=0.6315 | bad=0/10\n",
            "Epoch 002 | loss=0.4876 | val_auc=0.6319 | bad=0/10\n",
            "Epoch 003 | loss=0.4829 | val_auc=0.6376 | bad=0/10\n",
            "Epoch 004 | loss=0.4803 | val_auc=0.6402 | bad=0/10\n",
            "Epoch 005 | loss=0.4778 | val_auc=0.6374 | bad=1/10\n",
            "Epoch 006 | loss=0.4753 | val_auc=0.6438 | bad=0/10\n",
            "Epoch 007 | loss=0.4745 | val_auc=0.6463 | bad=0/10\n",
            "Epoch 008 | loss=0.4712 | val_auc=0.6524 | bad=0/10\n",
            "Epoch 009 | loss=0.4703 | val_auc=0.6568 | bad=0/10\n",
            "Epoch 010 | loss=0.4678 | val_auc=0.6588 | bad=0/10\n",
            "Epoch 011 | loss=0.4677 | val_auc=0.6631 | bad=0/10\n",
            "Epoch 012 | loss=0.4648 | val_auc=0.6646 | bad=0/10\n",
            "Epoch 013 | loss=0.4630 | val_auc=0.6645 | bad=1/10\n",
            "Epoch 014 | loss=0.4611 | val_auc=0.6617 | bad=2/10\n",
            "Epoch 015 | loss=0.4615 | val_auc=0.6654 | bad=0/10\n",
            "Epoch 016 | loss=0.4612 | val_auc=0.6716 | bad=0/10\n",
            "Epoch 017 | loss=0.4600 | val_auc=0.6700 | bad=1/10\n",
            "Epoch 018 | loss=0.4579 | val_auc=0.6739 | bad=0/10\n",
            "Epoch 019 | loss=0.4541 | val_auc=0.6714 | bad=1/10\n",
            "Epoch 020 | loss=0.4538 | val_auc=0.6743 | bad=0/10\n",
            "Epoch 021 | loss=0.4527 | val_auc=0.6764 | bad=0/10\n",
            "Epoch 022 | loss=0.4508 | val_auc=0.6743 | bad=1/10\n",
            "Epoch 023 | loss=0.4494 | val_auc=0.6774 | bad=0/10\n",
            "Epoch 024 | loss=0.4481 | val_auc=0.6716 | bad=1/10\n",
            "Epoch 025 | loss=0.4483 | val_auc=0.6776 | bad=0/10\n",
            "Epoch 026 | loss=0.4457 | val_auc=0.6777 | bad=0/10\n",
            "Epoch 027 | loss=0.4447 | val_auc=0.6757 | bad=1/10\n",
            "Epoch 028 | loss=0.4432 | val_auc=0.6760 | bad=2/10\n",
            "Epoch 029 | loss=0.4417 | val_auc=0.6804 | bad=0/10\n",
            "Epoch 030 | loss=0.4383 | val_auc=0.6781 | bad=1/10\n",
            "Epoch 031 | loss=0.4393 | val_auc=0.6780 | bad=2/10\n",
            "Epoch 032 | loss=0.4375 | val_auc=0.6779 | bad=3/10\n",
            "Epoch 033 | loss=0.4364 | val_auc=0.6777 | bad=4/10\n",
            "Epoch 034 | loss=0.4338 | val_auc=0.6791 | bad=5/10\n",
            "Epoch 035 | loss=0.4323 | val_auc=0.6735 | bad=6/10\n",
            "Epoch 036 | loss=0.4319 | val_auc=0.6728 | bad=7/10\n",
            "Epoch 037 | loss=0.4311 | val_auc=0.6761 | bad=8/10\n",
            "Epoch 038 | loss=0.4297 | val_auc=0.6796 | bad=9/10\n",
            "Epoch 039 | loss=0.4266 | val_auc=0.6767 | bad=10/10\n",
            "{'best_epoch': 29, 'best_val_auc': 0.6804265565875474, 'val_f1_at_thr_star': 0.748546511627907, 'thr_star_from_val': 0.3005527638190955, 'pos_weight': 0.5684582189093762}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def predict_test_probs(model, data_test, device):\n",
        "  model.eval()\n",
        "  d = data_test.to(device)\n",
        "  logits = model(d).view(-1)\n",
        "  probs = torch.sigmoid(logits).cpu().numpy()\n",
        "  return probs"
      ],
      "metadata": {
        "id": "NGI56BfksQ5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_probs = predict_test_probs(model, data_test, device)\n",
        "print(test_probs.shape)   # (num_test_rows,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k18POc2zsEee",
        "outputId": "68be3400-e8ea-4892-ec0a-83ffeb963c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(825,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task.evaluate(test_probs) # just GNN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw0rZzVIpvTt",
        "outputId": "cfeb2219-8787-48aa-d3a1-0d1b3f227855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'average_precision': np.float64(0.7761568328505457),\n",
              " 'accuracy': 0.6339393939393939,\n",
              " 'f1': 0.6447058823529411,\n",
              " 'roc_auc': np.float64(0.7167919799498748)}"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fuse embeddings with tabular"
      ],
      "metadata": {
        "id": "1HSCWrbuusrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_embeddings(df: pd.DataFrame, emb: \"np.ndarray\", prefix: str = \"gnn_\") -> pd.DataFrame:\n",
        "    emb_df = pd.DataFrame(emb, columns=[f\"{prefix}{i}\" for i in range(emb.shape[1])])\n",
        "    return pd.concat([df.reset_index(drop=True), emb_df], axis=1)\n",
        "\n",
        "df_train_aug = add_embeddings(df_train, emb_train)\n",
        "df_val_aug   = add_embeddings(df_val,   emb_val)\n",
        "df_test_aug  = add_embeddings(df_test,  emb_test)"
      ],
      "metadata": {
        "id": "4wTai3NfundQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "\n",
        "path = \"/Models/\"\n",
        "\n",
        "predictor = TabularPredictor(\n",
        "    label=\"outcome\",\n",
        "    path=path,\n",
        "    eval_metric=\"roc_auc\",\n",
        ").fit(\n",
        "    train_data=df_train_aug,\n",
        "    tuning_data=df_val_aug,\n",
        "    time_limit=3600,\n",
        "    presets=\"medium_quality\",   # medium is enough to beat RDL, RGT\n",
        "    included_model_types=[\n",
        "        \"GBM\",      # LightGBM\n",
        "        \"CAT\",      # CatBoost\n",
        "        \"XGB\",      # XGBoost\n",
        "        \"RF\",       # optional\n",
        "        \"XT\",       # optional\n",
        "        \"REALMLP\",  # MLP (if available)\n",
        "        # optionally: \"NN_TORCH\" (depending on your AG version/install)\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CFz06mmu3-Q",
        "outputId": "578a3040-d191-42ed-894f-81213b51e9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"/content/drive/MyDrive/ICML-transactions/trials_data/Models/Tabular/tabular+gnn(T)/\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.5.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Pytorch Version:    2.4.1+cu121\n",
            "CUDA Version:       12.1\n",
            "GPU Memory:         GPU 0: 14.70/14.74 GB\n",
            "Total GPU Memory:   Free: 14.70 GB, Allocated: 0.04 GB, Total: 14.74 GB\n",
            "GPU Count:          1\n",
            "Memory Avail:       7.19 GB / 12.67 GB (56.8%)\n",
            "Disk Space Avail:   173.27 GB / 235.68 GB (73.5%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Using hyperparameters preset: hyperparameters='default'\n",
            "Beginning AutoGluon training ... Time limit = 3600s\n",
            "AutoGluon will save models to \"/content/drive/MyDrive/ICML-transactions/trials_data/Models/Tabular/tabular+gnn(T)\"\n",
            "Train Data Rows:    11994\n",
            "Train Data Columns: 94\n",
            "Tuning Data Rows:    960\n",
            "Tuning Data Columns: 94\n",
            "Label Column:       outcome\n",
            "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
            "\t2 unique label values:  [np.int32(1), np.int32(0)]\n",
            "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    7396.65 MB\n",
            "\tTrain Data (Original)  Memory Usage: 40.16 MB (0.5% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\t\tFitting DatetimeFeatureGenerator...\n",
            "\t\tFitting TextSpecialFeatureGenerator...\n",
            "\t\t\tFitting BinnedFeatureGenerator...\n",
            "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\t\tFitting TextNgramFeatureGenerator...\n",
            "\t\t\tFitting CountVectorizer for text features: ['baseline_population', 'brief_title', 'official_title', 'source', 'detailed_descriptions', 'brief_summaries']\n",
            "\t\t\tCountVectorizer fit with vocabulary size = 10000\n",
            "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
            "\t\tReducing Vectorizer vocab size from 10000 to 2405 to avoid OOM error\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 3): ['enrollment_type', 'limitations_and_caveats', 'is_ppsd']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('datetime', [])     :  2 | ['timestamp', 'start_date']\n",
            "\t\t('float', [])        : 67 | ['enrollment', 'number_of_arms', 'number_of_groups', 'gnn_0', 'gnn_1', ...]\n",
            "\t\t('int', [])          :  1 | ['nct_id']\n",
            "\t\t('object', [])       : 15 | ['target_duration', 'study_type', 'acronym', 'phase', 'has_dmc', ...]\n",
            "\t\t('object', ['text']) :  6 | ['baseline_population', 'brief_title', 'official_title', 'source', 'detailed_descriptions', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])                    :   13 | ['target_duration', 'study_type', 'acronym', 'phase', 'has_dmc', ...]\n",
            "\t\t('category', ['text_as_category'])  :    6 | ['baseline_population', 'brief_title', 'official_title', 'source', 'detailed_descriptions', ...]\n",
            "\t\t('float', [])                       :   67 | ['enrollment', 'number_of_arms', 'number_of_groups', 'gnn_0', 'gnn_1', ...]\n",
            "\t\t('int', [])                         :    1 | ['nct_id']\n",
            "\t\t('int', ['binned', 'text_special']) :  111 | ['baseline_population.char_count', 'baseline_population.word_count', 'baseline_population.capital_ratio', 'baseline_population.lower_ratio', 'baseline_population.digit_ratio', ...]\n",
            "\t\t('int', ['bool'])                   :    2 | ['is_unapproved_device', 'fdaaa801_violation']\n",
            "\t\t('int', ['datetime_as_int'])        :    9 | ['timestamp', 'timestamp.year', 'timestamp.day', 'timestamp.dayofweek', 'start_date', ...]\n",
            "\t\t('int', ['text_ngram'])             : 2396 | ['__nlp__.000', '__nlp__.05', '__nlp__.10', '__nlp__.10 mg', '__nlp__.100', ...]\n",
            "\t52.4s = Fit runtime\n",
            "\t91 features in original data used to generate 2605 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 65.32 MB (0.9% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 53.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
            "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'FASTAI': [{}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "Included models: ['GBM', 'CAT', 'XGB', 'RF', 'XT', 'REALMLP'] (Specified by `included_model_types`, all other model types will be skipped)\n",
            "\tThe models types ['REALMLP'] are not present in the model list specified by the user and will be ignored:\n",
            "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT ... Training model for up to 3546.79s of the 3546.78s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.8/7.1 GB\n",
            "\t0.692\t = Validation score   (roc_auc)\n",
            "\t17.14s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 3529.55s of the 3529.54s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.8/7.1 GB\n",
            "\t0.6902\t = Validation score   (roc_auc)\n",
            "\t14.4s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ... Training model for up to 3515.04s of the 3515.04s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0, mem=0.1/7.2 GB\n",
            "\t0.6921\t = Validation score   (roc_auc)\n",
            "\t37.91s\t = Training   runtime\n",
            "\t0.22s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ... Training model for up to 3476.60s of the 3476.60s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0, mem=0.1/7.2 GB\n",
            "\t0.6939\t = Validation score   (roc_auc)\n",
            "\t36.95s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 3439.24s of the 3439.23s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=3.0/7.2 GB\n",
            "\t0.6943\t = Validation score   (roc_auc)\n",
            "\t124.5s\t = Training   runtime\n",
            "\t0.16s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ... Training model for up to 3314.53s of the 3314.52s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0, mem=0.1/6.9 GB\n",
            "\t0.6741\t = Validation score   (roc_auc)\n",
            "\t34.1s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ... Training model for up to 3279.87s of the 3279.87s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0, mem=0.1/6.8 GB\n",
            "\t0.6898\t = Validation score   (roc_auc)\n",
            "\t37.2s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 3242.12s of the 3242.12s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=1.4/6.8 GB\n",
            "\t0.6781\t = Validation score   (roc_auc)\n",
            "\t61.1s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 3180.90s of the 3180.89s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=2.3/6.7 GB\n",
            "\t0.6865\t = Validation score   (roc_auc)\n",
            "\t40.55s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 3140.13s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/6.7 GB\n",
            "\tEnsemble Weights: {'ExtraTreesEntr': 0.471, 'CatBoost': 0.294, 'LightGBMXT': 0.176, 'RandomForestEntr': 0.059}\n",
            "\t0.7011\t = Validation score   (roc_auc)\n",
            "\t0.07s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 460.32s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1938.9 rows/s (960 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/drive/MyDrive/ICML-transactions/trials_data/Models/Tabular/tabular+gnn(T)\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictor.leaderboard(df_val_aug, silent=True).head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKcnYEyYvq5Q",
        "outputId": "61d553d4-0633-4711-98b2-7036fb61f692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 model  score_test  score_val eval_metric  pred_time_test  \\\n",
            "0  WeightedEnsemble_L2    0.701129   0.701129     roc_auc        0.939949   \n",
            "1             CatBoost    0.694276   0.694276     roc_auc        0.115787   \n",
            "2     RandomForestEntr    0.693918   0.693918     roc_auc        0.307346   \n",
            "3     RandomForestGini    0.692134   0.692134     roc_auc        0.352164   \n",
            "4           LightGBMXT    0.692024   0.692024     roc_auc        0.108782   \n",
            "5             LightGBM    0.690157   0.690157     roc_auc        0.100283   \n",
            "6       ExtraTreesEntr    0.689819   0.689819     roc_auc        0.402246   \n",
            "7        LightGBMLarge    0.686471   0.686471     roc_auc        0.086230   \n",
            "8              XGBoost    0.678065   0.678065     roc_auc        0.105120   \n",
            "9       ExtraTreesGini    0.674067   0.674067     roc_auc        0.391507   \n",
            "\n",
            "   pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
            "0       0.495127  215.847269                 0.005788                0.000860   \n",
            "1       0.159963  124.496386                 0.115787                0.159963   \n",
            "2       0.122074   36.946317                 0.307346                0.122074   \n",
            "3       0.217267   37.913848                 0.352164                0.217267   \n",
            "4       0.060854   17.136996                 0.108782                0.060854   \n",
            "5       0.057016   14.398999                 0.100283                0.057016   \n",
            "6       0.151376   37.197680                 0.402246                0.151376   \n",
            "7       0.097626   40.545727                 0.086230                0.097626   \n",
            "8       0.066867   61.102702                 0.105120                0.066867   \n",
            "9       0.154841   34.101013                 0.391507                0.154841   \n",
            "\n",
            "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
            "0           0.069890            2       True         10  \n",
            "1         124.496386            1       True          5  \n",
            "2          36.946317            1       True          4  \n",
            "3          37.913848            1       True          3  \n",
            "4          17.136996            1       True          1  \n",
            "5          14.398999            1       True          2  \n",
            "6          37.197680            1       True          7  \n",
            "7          40.545727            1       True          9  \n",
            "8          61.102702            1       True          8  \n",
            "9          34.101013            1       True          6  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proba  = predictor.predict_proba(df_test_aug)\n",
        "preds_proba = proba[1]\n",
        "\n",
        "results = task.evaluate(preds_proba)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpHntYcUvyBS",
        "outputId": "6173e774-4520-47ef-b592-843cf64dbe9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'average_precision': np.float64(0.7800480047352947),\n",
              " 'accuracy': 0.6727272727272727,\n",
              " 'f1': 0.755877034358047,\n",
              " 'roc_auc': np.float64(0.724238131560786)}"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_PATH = '/runs/'\n",
        "\n",
        "df_tab_GNN_T = pd.DataFrame([results])\n",
        "df_tab_GNN_T[\"model\"] = \"Tab+GNN(T)\"\n",
        "df_tab_GNN_T[\"task\"] = task\n",
        "\n",
        "df_tab_GNN_T.to_csv(OUTPUT_PATH+f\"Tab-GNN(T)_{task}.csv\")"
      ],
      "metadata": {
        "id": "gp-xj3XqwuEx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}