{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5azeWnuL2jF3"
      },
      "outputs": [],
      "source": [
        "!pip install relbench --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"autogluon.tabular[all]\""
      ],
      "metadata": {
        "id": "yqb8icjaJ25W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from relbench.datasets import get_dataset\n",
        "\n",
        "dataset = get_dataset(name=\"rel-trial\", download=True)"
      ],
      "metadata": {
        "id": "1JgM4zkd2tZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from relbench.tasks import get_task\n",
        "\n",
        "db = dataset.get_db()\n",
        "\n",
        "table = db.table_dict[\"studies\"]\n",
        "task = get_task(\"rel-trial\", \"study-outcome\", download=True)\n",
        "\n",
        "train_task = task.get_table(\"train\").df\n",
        "val_task = task.get_table(\"val\").df\n",
        "test_task = task.get_table(\"test\").df\n",
        "\n",
        "## learning table\n",
        "\n",
        "train_df = train_task.merge(table.df, how=\"left\")\n",
        "val_df = val_task.merge(table.df, how=\"left\")\n",
        "test_df = test_task.merge(table.df, how=\"left\")"
      ],
      "metadata": {
        "id": "S2pQPtR423Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_PATH = '/run/'"
      ],
      "metadata": {
        "id": "KrojrDw86q3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tabular NFA"
      ],
      "metadata": {
        "id": "3Q4myJln5u5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.set_index(\"nct_id\", drop=False)\n",
        "val_df = val_df.set_index(\"nct_id\", drop=False)\n",
        "test_df= test_df.set_index(\"nct_id\", drop=False)\n",
        "\n",
        "\n",
        "# sanity check\n",
        "assert train_df.index.is_unique and val_df.index.is_unique and test_df.index.is_unique\n",
        "assert train_df.index.intersection(val_df.index).empty\n",
        "assert train_df.index.intersection(test_df.index).empty\n",
        "assert val_df.index.intersection(test_df.index).empty"
      ],
      "metadata": {
        "id": "Od1b-zGG5am4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Optional, Tuple, Union, Dict\n",
        "\n",
        "IndexLike = Union[pd.Index, pd.Series, List[int]]\n",
        "\n",
        "def nfa_or_time_window_props(\n",
        "    T: pd.DataFrame,\n",
        "    df_idx: IndexLike,\n",
        "    ref_idx: IndexLike,\n",
        "    *,\n",
        "    time_col: str,\n",
        "    window: Union[str, pd.Timedelta],\n",
        "    group_cols: List[str],\n",
        "    numeric_cols: Optional[List[str]] = None,\n",
        "    categorical_cols: Optional[List[str]] = None,\n",
        "    prefix: str = \"nfa\",\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    OR-neighborhood NFA with trailing time window and categorical proportions.\n",
        "\n",
        "    Neighbors(i) = union over c in group_cols of:\n",
        "        rows j in ref_idx where\n",
        "          T[j,c] == T[i,c] AND\n",
        "          t_i - window <= t_j < t_i\n",
        "\n",
        "    Categorical outputs are proportions (counts / neighbor_count).\n",
        "    \"\"\"\n",
        "    numeric_cols = list(numeric_cols or [])\n",
        "    categorical_cols = list(categorical_cols or [])\n",
        "    if isinstance(window, str):\n",
        "        window = pd.Timedelta(window)\n",
        "\n",
        "    df = T.loc[pd.Index(df_idx)].copy()\n",
        "    ref_df = T.loc[pd.Index(ref_idx)]\n",
        "\n",
        "    df_time = pd.to_datetime(df[time_col])\n",
        "    ref_time = pd.to_datetime(ref_df[time_col]).to_numpy(dtype=\"datetime64[ns]\")\n",
        "\n",
        "    # group lookups: value -> ref positions\n",
        "    group_maps: Dict[str, Dict[object, np.ndarray]] = {\n",
        "        c: ref_df.groupby(c, sort=False).indices for c in group_cols\n",
        "    }\n",
        "\n",
        "    # numeric reference matrix\n",
        "    ref_num = ref_df[numeric_cols].to_numpy(dtype=float) if numeric_cols else None\n",
        "\n",
        "    # categorical metadata (freeze levels on reference)\n",
        "    cat_meta = {}\n",
        "    for c in categorical_cols:\n",
        "        levels = pd.Index(pd.unique(ref_df[c].dropna()))\n",
        "        codes = pd.Categorical(ref_df[c], categories=levels).codes.astype(np.int32)\n",
        "        cat_meta[c] = {\"levels\": levels, \"codes\": codes, \"k\": len(levels)}\n",
        "\n",
        "    n = len(df)\n",
        "    neigh_count = np.zeros(n, dtype=np.int64)\n",
        "\n",
        "    if numeric_cols:\n",
        "        mean_out = np.full((n, len(numeric_cols)), np.nan)\n",
        "        min_out  = np.full((n, len(numeric_cols)), np.nan)\n",
        "        max_out  = np.full((n, len(numeric_cols)), np.nan)\n",
        "\n",
        "    cat_prop_blocks = {\n",
        "        c: np.full((n, meta[\"k\"]), np.nan, dtype=float)\n",
        "        for c, meta in cat_meta.items()\n",
        "    }\n",
        "\n",
        "    for i, (idx, row) in enumerate(df.iterrows()):\n",
        "        t_i = np.datetime64(df_time.iloc[i].to_datetime64())\n",
        "\n",
        "        parts = []\n",
        "        for c in group_cols:\n",
        "            arr = group_maps[c].get(row[c])\n",
        "            if arr is not None:\n",
        "                parts.append(arr)\n",
        "        if not parts:\n",
        "            continue\n",
        "\n",
        "        neigh_pos = np.unique(np.concatenate(parts))\n",
        "\n",
        "        # time window filter\n",
        "        t_lo = t_i - np.timedelta64(window.value, \"ns\")\n",
        "        t_ref = ref_time[neigh_pos]\n",
        "        mask = (t_ref >= t_lo) & (t_ref < t_i)\n",
        "        neigh_pos = neigh_pos[mask]\n",
        "\n",
        "        if len(neigh_pos) == 0:\n",
        "            continue\n",
        "\n",
        "        neigh_count[i] = len(neigh_pos)\n",
        "\n",
        "        if numeric_cols:\n",
        "            vals = ref_num[neigh_pos, :]\n",
        "            with np.errstate(invalid=\"ignore\"):\n",
        "                mean_out[i, :] = np.nanmean(vals, axis=0)\n",
        "                min_out[i, :]  = np.nanmin(vals, axis=0)\n",
        "                max_out[i, :]  = np.nanmax(vals, axis=0)\n",
        "\n",
        "        for c, meta in cat_meta.items():\n",
        "            codes = meta[\"codes\"][neigh_pos]\n",
        "            codes = codes[codes >= 0]\n",
        "            if codes.size:\n",
        "                counts = np.bincount(codes, minlength=meta[\"k\"])\n",
        "                cat_prop_blocks[c][i, :] = counts / neigh_count[i]\n",
        "\n",
        "    out = [df]\n",
        "\n",
        "    out.append(pd.DataFrame(\n",
        "        {f\"{prefix}_neighbor_count\": neigh_count},\n",
        "        index=df.index\n",
        "    ))\n",
        "\n",
        "    if numeric_cols:\n",
        "        out.append(pd.DataFrame(\n",
        "            np.column_stack([mean_out, min_out, max_out]),\n",
        "            index=df.index,\n",
        "            columns=[\n",
        "                *(f\"{prefix}_{c}_mean\" for c in numeric_cols),\n",
        "                *(f\"{prefix}_{c}_min\"  for c in numeric_cols),\n",
        "                *(f\"{prefix}_{c}_max\"  for c in numeric_cols),\n",
        "            ],\n",
        "        ))\n",
        "\n",
        "    for c, meta in cat_meta.items():\n",
        "        cols = [f\"{prefix}_prop_{c}__{lvl}\" for lvl in meta[\"levels\"].astype(str)]\n",
        "        out.append(pd.DataFrame(cat_prop_blocks[c], index=df.index, columns=cols))\n",
        "\n",
        "    return pd.concat(out, axis=1)"
      ],
      "metadata": {
        "id": "TQwZhGXs77Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_nfa_and_trim_train(\n",
        "  df_train: pd.DataFrame,\n",
        "  df_val: pd.DataFrame,\n",
        "  df_test: pd.DataFrame,\n",
        "  *,\n",
        "  time_col: str,\n",
        "  window: Union[str, pd.Timedelta],\n",
        "  group_cols: List[str],\n",
        "  numeric_cols: Optional[List[str]] = None,\n",
        "  categorical_cols: Optional[List[str]] = None,\n",
        "  train_drop_before: Union[str, pd.Timestamp], # e.g. \"2001-01-01\"\n",
        "  prefix: str = \"nfa\",\n",
        "  ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "  \"\"\"\n",
        "  - Computes NFA features on train / val / test\n",
        "  - Uses early train rows as NFA context\n",
        "  - Drops early train rows from supervised training\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  T = pd.concat([df_train, df_val, df_test], axis=0)\n",
        "  T = T.sort_values(time_col)\n",
        "\n",
        "\n",
        "  tr_idx = df_train.index\n",
        "  va_idx = df_val.index\n",
        "  te_idx = df_test.index\n",
        "\n",
        "\n",
        "  # TRAIN: neighbors from past train\n",
        "  tr_nfa = nfa_or_time_window_props(\n",
        "  T, tr_idx, tr_idx,\n",
        "  time_col=time_col,\n",
        "  window=window,\n",
        "  group_cols=group_cols,\n",
        "  numeric_cols=numeric_cols,\n",
        "  categorical_cols=categorical_cols,\n",
        "  prefix=prefix,\n",
        "  )\n",
        "\n",
        "\n",
        "  # VAL: neighbors from past train + past val\n",
        "  va_ref = tr_idx.union(va_idx)\n",
        "  va_nfa = nfa_or_time_window_props(\n",
        "  T, va_idx, va_ref,\n",
        "  time_col=time_col,\n",
        "  window=window,\n",
        "  group_cols=group_cols,\n",
        "  numeric_cols=numeric_cols,\n",
        "  categorical_cols=categorical_cols,\n",
        "  prefix=prefix,\n",
        "  )\n",
        "\n",
        "\n",
        "  # TEST: neighbors from past train + past val + past test\n",
        "  te_ref = tr_idx.union(va_idx).union(te_idx)\n",
        "  te_nfa = nfa_or_time_window_props(\n",
        "  T, te_idx, te_ref,\n",
        "  time_col=time_col,\n",
        "  window=window,\n",
        "  group_cols=group_cols,\n",
        "  numeric_cols=numeric_cols,\n",
        "  categorical_cols=categorical_cols,\n",
        "  prefix=prefix,\n",
        "  )\n",
        "\n",
        "\n",
        "  # Trim early training rows (used only as NFA context)\n",
        "  cutoff = pd.to_datetime(train_drop_before)\n",
        "  keep_mask = pd.to_datetime(tr_nfa[time_col]) >= cutoff\n",
        "  tr_nfa_trimmed = tr_nfa.loc[keep_mask]\n",
        "\n",
        "\n",
        "  return tr_nfa_trimmed, va_nfa, te_nfa"
      ],
      "metadata": {
        "id": "CKsH1kfYw9hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_nfa, df_val_nfa, df_test_nfa = add_nfa_and_trim_train(\n",
        "    train_df,\n",
        "    val_df,\n",
        "    test_df,\n",
        "    time_col=\"start_date\",\n",
        "    window=\"365D\",  # 1 year trailing window\n",
        "    group_cols=[\n",
        "        \"phase\",\n",
        "        \"study_type\",\n",
        "        \"source_class\",\n",
        "        \"enrollment_type\",\n",
        "        \"has_dmc\",\n",
        "    ],\n",
        "    numeric_cols=[\n",
        "        \"enrollment\",\n",
        "        \"number_of_arms\",\n",
        "        \"number_of_groups\",\n",
        "        \"limitations_and_caveats\",\n",
        "    ],\n",
        "    categorical_cols=[\n",
        "        \"phase\",\n",
        "        \"study_type\",\n",
        "        \"enrollment_type\",\n",
        "        \"has_dmc\",\n",
        "        \"is_fda_regulated_drug\",\n",
        "        \"is_fda_regulated_device\",\n",
        "        \"is_unapproved_device\",\n",
        "        \"is_ppsd\",\n",
        "        \"is_us_export\",\n",
        "        \"biospec_retention\",\n",
        "        \"source_class\",\n",
        "        \"fdaaa801_violation\",\n",
        "        \"plan_to_share_ipd\",\n",
        "    ],\n",
        "    train_drop_before=\"2001-01-01\",  # bootstrap year used only as NFA context\n",
        ")"
      ],
      "metadata": {
        "id": "8uG6kHUIxGlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1) take training schema as the only schema\n",
        "train_cols = df_train_nfa.columns\n",
        "\n",
        "# 2) reindex val and test to training schema\n",
        "df_val_nfa  = df_val_nfa.reindex(columns=train_cols)\n",
        "df_test_nfa = df_test_nfa.reindex(columns=train_cols)\n",
        "\n",
        "# (train already has the correct schema)\n",
        "# df_train_nfa = df_train_nfa.reindex(columns=train_cols)  # optional / no-op\n",
        "\n",
        "# 3) fill categorical proportions with 0 (correct semantics)\n",
        "prop_cols = [c for c in train_cols if c.startswith(\"nfa_prop_\")]\n",
        "df_val_nfa[prop_cols]  = df_val_nfa[prop_cols].fillna(0.0)\n",
        "df_test_nfa[prop_cols] = df_test_nfa[prop_cols].fillna(0.0)\n",
        "\n",
        "# 4) fill neighbor count if ever missing\n",
        "if \"nfa_neighbor_count\" in train_cols:\n",
        "    df_val_nfa[\"nfa_neighbor_count\"]  = df_val_nfa[\"nfa_neighbor_count\"].fillna(0).astype(int)\n",
        "    df_test_nfa[\"nfa_neighbor_count\"] = df_test_nfa[\"nfa_neighbor_count\"].fillna(0).astype(int)\n",
        "\n",
        "# 5) final guarantee\n",
        "assert df_train_nfa.columns.equals(df_val_nfa.columns)\n",
        "assert df_train_nfa.columns.equals(df_test_nfa.columns)"
      ],
      "metadata": {
        "id": "Jbc8oQ427Ubb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "\n",
        "predictor = TabularPredictor(\n",
        "    label=\"outcome\",\n",
        "    path=path+\"/Tabular/tabular+nfa/\",\n",
        "    eval_metric=\"roc_auc\",  # or \"f1\",\n",
        ").fit(\n",
        "    train_data=df_train_nfa,\n",
        "    tuning_data=df_val_nfa,\n",
        "    time_limit=3600,\n",
        "    presets=\"medium_quality\",\n",
        "    included_model_types=[\n",
        "        \"GBM\",      # LightGBM\n",
        "        \"CAT\",      # CatBoost\n",
        "        \"XGB\",      # XGBoost\n",
        "        \"RF\",       # optional\n",
        "        \"XT\",       # optional\n",
        "        \"REALMLP\",  # MLP (if available)\n",
        "    ],\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "fnP7jueLzpl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.leaderboard(df_val_nfa, silent=True).head(20) # Top 20 models"
      ],
      "metadata": {
        "id": "UGPuFtfR-lTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proba  = predictor.predict_proba(df_test_nfa)\n",
        "preds_proba = proba[1]\n",
        "\n",
        "results = task.evaluate(preds_proba)"
      ],
      "metadata": {
        "id": "R5ebPBre1HME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tab_nfa = pd.DataFrame([results])\n",
        "df_tab_nfa[\"model\"] = \"Tab+NFA\"\n",
        "df_tab_nfa[\"task\"] = task\n",
        "\n",
        "df_tab_nfa.to_csv(OUTPUT_PATH+f\"Tab+NFA_Trail_{task}.csv\")"
      ],
      "metadata": {
        "id": "6TufDoCoKQL1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}